{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdeac8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: numpy in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (1.23.0)\n",
      "Requirement already satisfied: scipy in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (1.8.1)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (4.64.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.2\n",
      "  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Using cached cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ac71af753f3c1160c6a0b3f5007c35795d87af60b0b217ad8b4d6a86beacf5a8\n",
      "  Stored in directory: /Users/przivic/Library/Caches/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "Successfully built future\n",
      "Installing collected packages: py4j, networkx, future, cloudpickle, hyperopt\n",
      "Successfully installed cloudpickle-2.1.0 future-0.18.2 hyperopt-0.2.7 networkx-2.8.4 py4j-0.10.9.5\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e5be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-3.3.2-py3-none-macosx_10_14_x86_64.macosx_10_15_x86_64.macosx_11_0_x86_64.whl (1.2 MB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.23.1-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.1 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Using cached scipy-1.8.1-cp39-cp39-macosx_12_0_universal2.macosx_10_9_x86_64.whl (55.6 MB)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92093c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16976a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pct(train)': 0.7837289649483001, 'pct(dev)': 0.11952685477518159, 'pct(test)': 0.09674418027651828}\n"
     ]
    }
   ],
   "source": [
    "from taller_model_selection.evaluate import load_train_dev_test\n",
    "\n",
    "(X_train, y_train), (X_dev, y_dev), test = load_train_dev_test('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b98763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taller_model_selection.metrics import rmse\n",
    "from taller_model_selection.transformers import FeatureProjection\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "features_pipe = make_union(\n",
    "    make_pipeline(\n",
    "        FeatureProjection(['rooms', 'bedrooms', 'bathrooms', 'surface_total', 'surface_covered']),\n",
    "        SimpleImputer()\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        FeatureProjection(['l3']), \n",
    "        SimpleImputer(strategy='most_frequent'),\n",
    "        OneHotEncoder(sparse=False)\n",
    "    ), \n",
    "    make_pipeline(\n",
    "        FeatureProjection(['l4']), \n",
    "        SimpleImputer(strategy='constant'),\n",
    "        OneHotEncoder(sparse=False)\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba580eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from pprint import pprint\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from time import time\n",
    "\n",
    "def eval_pipe(model_name, pipe):\n",
    "    return dict(\n",
    "        name=model_name,\n",
    "        train=rmse(y_train, pipe.predict(X_train)),\n",
    "        dev=rmse(y_dev, pipe.predict(X_dev))\n",
    "    )\n",
    "\n",
    "def objective(params):\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    print(params)\n",
    "    pipe = make_pipeline(\n",
    "        features_pipe,\n",
    "        lgb.LGBMRegressor(random_state=42, **params)\n",
    "    )\n",
    "    t0 = time()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    loss=rmse(y_dev, pipe.predict(X_dev))\n",
    "    print(f'loss {loss:.02f}')\n",
    "    return dict(\n",
    "        loss=loss,\n",
    "        tr_loss=rmse(y_train, pipe.predict(X_train)), \n",
    "        params=params,\n",
    "        train_time=train_time,\n",
    "        status=STATUS_OK\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9c804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 3000, 10),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1.0, 0.1),\n",
    "    'objective': hp.choice('objective', [\"regression\", \"regression_l1\"]),\n",
    "    'learning_rate': hp.qloguniform('learning_rate', np.log(0.01), np.log(0.3), 0.01),\n",
    "    'reg_alpha': hp.choice('ra', [0, hp.quniform('reg_alpha', 0.01, 0.1, 0.01)]),\n",
    "}\n",
    "\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1cd5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.27, 'n_estimators': 2050, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 297491.03                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1980, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.5}\n",
      "loss 297508.31                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.6000000000000001}\n",
      "loss 282722.61                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 290, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.7000000000000001}\n",
      "loss 288252.66                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2850, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.6000000000000001}\n",
      "loss 295902.03                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1380, 'objective': 'regression_l1', 'reg_alpha': 0.09, 'subsample': 0.6000000000000001}\n",
      "loss 297283.65                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 285564.19                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.6000000000000001}\n",
      "loss 281552.12                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2940, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 295894.92                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.6000000000000001}\n",
      "loss 285247.90                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 285162.74                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281444.87                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 630, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 319996.17                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2590, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 296161.67                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 286318.37                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2760, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 294526.82                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281825.25                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2190, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 297294.34                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 2050, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 297491.03                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 282742.65                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1020, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 285668.61                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 282059.33                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 284213.57                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 281876.27                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 830, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 283745.16                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.5}\n",
      "loss 282405.74                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282053.68                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 1.0}\n",
      "loss 284182.72                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281326.12                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284417.87                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281537.00                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 284519.16                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 293896.52                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284637.94                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 283922.91                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283639.19                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281896.19                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 400, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 286602.38                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1860, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 314367.23                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281399.89                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 284541.13                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 20, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 392564.85                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281965.07                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282644.41                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1360, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 294494.53                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281725.71                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283013.52                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2480, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 297199.21                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 282284.90                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1500, 'objective': 'regression_l1', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 304064.54                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282062.69                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 285212.06                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281410.42                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2140, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 294296.55                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282164.21                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2000, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 284563.37                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282543.96                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2270, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 295036.81                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 940, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 283826.87                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281940.44                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 287280.58                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 100, 'objective': 'regression_l1', 'reg_alpha': 0.1, 'subsample': 0.5}\n",
      "loss 425801.19                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 286126.77                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 300, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283586.01                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283599.62                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281620.03                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 3000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281083.49                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281601.96                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281505.46                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 284377.23                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282478.57                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281805.19                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280499.09                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280532.02                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282299.96                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281270.51                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 284504.68                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284059.01                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282245.08                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2420, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 294133.34                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281121.05                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283093.26                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 1.0}\n",
      "loss 281199.74                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2570, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 293203.16                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.04, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282394.55                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282817.28                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280919.83                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 281113.94                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1790, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 294407.54                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281185.76                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283314.40                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.8}\n",
      "loss 281371.76                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2200, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 293285.54                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281487.60                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283111.84                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283041.97                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2620, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 284361.85                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1410, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 296097.10                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280615.05                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280615.05                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283864.88                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 940, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 282278.58                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1300, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 294755.56                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283346.22                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281726.29                                                                  \n",
      "{'learning_rate': 0.29, 'n_estimators': 540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283042.08                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 690, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 282300.67                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1250, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 298232.47                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282591.96                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282708.38                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 286256.00                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1160, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 282157.86                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1710, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 294490.05                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281834.42                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281343.31                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281634.58                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 282592.17                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2110, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 294428.87                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281780.56                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281801.67                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281801.03                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281607.40                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 860, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 285549.84                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1410, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 294558.84                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281350.22                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281923.13                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 770, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 282226.84                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 170, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 285597.50                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1240, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 294959.68                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283724.83                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282203.82                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284862.29                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 281575.75                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2730, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 294996.97                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282761.55                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 283477.01                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282891.54                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1360, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.8}\n",
      "loss 283095.92                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1480, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 296342.93                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 1080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 285831.32                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2150, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282636.28                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 20, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300596.65                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.7000000000000001}\n",
      "loss 282195.21                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2030, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 295580.75                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282009.48                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282023.51                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282019.54                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284254.72                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283154.65                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1300, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282626.79                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280380.12                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281397.65                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281428.54                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281885.70                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281778.12                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280557.62                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281308.97                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282575.94                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283170.65                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 285768.69                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283291.28                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281420.18                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281495.81                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281881.28                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 1.0}\n",
      "loss 281425.61                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1900, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 295636.00                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282378.71                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 285606.08                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283490.81                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 282574.56                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 284234.38                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1760, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 294095.44                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282137.26                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 284872.88                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281658.17                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 1.0}\n",
      "loss 281381.94                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1480, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 295672.71                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281169.29                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282775.25                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282707.57                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282394.10                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2170, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 281180.53                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1250, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 297210.39                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281391.13                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281856.03                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281781.20                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283632.41                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 281397.65                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1030, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 294893.96                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281258.47                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281615.49                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282854.58                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282437.77                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 284853.77                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1930, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 295012.69                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281303.09                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281583.94                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282263.46                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282028.59                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 281095.85                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 1180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 297607.08                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282728.83                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281931.00                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281917.41                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281572.15                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 1.0}\n",
      "loss 282697.41                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 295505.17                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282223.51                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281693.85                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282259.52                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 285930.93                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 280952.51                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1650, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 295749.22                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283693.12                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1770, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281550.37                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1910, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 284373.08                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283752.47                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2700, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 282010.96                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2600, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 295235.57                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282176.38                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282298.70                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 288346.46                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283363.74                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1060, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 1.0}\n",
      "loss 282263.84                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1400, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 296508.78                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283033.29                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282570.85                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281156.78                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281314.91                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 680, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 284964.32                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281681.35                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2200, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 293285.54                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282944.59                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281814.42                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280749.26                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 281668.63                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284212.76                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1580, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 296531.50                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283678.11                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284753.65                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281332.28                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282426.50                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 850, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 283899.65                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1790, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 297718.87                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 300, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 285334.66                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282148.20                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283057.86                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281846.76                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2470, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 281364.27                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1440, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 294913.42                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283019.94                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280476.41                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282764.16                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283554.71                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 910, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 284532.55                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 640, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 301561.37                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 286187.39                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280250.35                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280438.88                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283222.42                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281952.05                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281568.99                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282791.07                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281854.50                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 285307.23                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 280575.42                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282012.92                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282947.65                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283518.86                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282420.18                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281598.60                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281842.30                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280476.41                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281912.44                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282060.78                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1300, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282626.79                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281456.44                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 284333.48                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282455.26                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 910, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282029.98                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282007.59                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283036.25                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282067.14                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282869.49                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283368.33                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.7000000000000001}\n",
      "loss 281929.95                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1350, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 297961.71                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280177.62                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281317.79                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282336.76                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281549.31                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.6000000000000001}\n",
      "loss 282064.08                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 860, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 297543.75                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 282215.21                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283349.31                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280381.57                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.11, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282725.40                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283803.54                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 282249.25                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1350, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 293862.23                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282591.96                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281944.24                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281791.69                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 285473.61                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282876.32                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 282366.05                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1790, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 294047.48                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282091.41                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 286493.06                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 285995.48                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283809.14                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 280381.57                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.5}\n",
      "loss 281572.64                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1740, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 295694.99                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 285737.41                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283503.89                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281487.59                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 282854.98                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 280370.89                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.5}\n",
      "loss 283356.54                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1830, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 293489.81                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281747.37                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 282066.53                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281800.05                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281658.91                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281850.88                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1140, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.5}\n",
      "loss 284124.73                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1650, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 295749.22                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282030.79                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281717.68                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 282706.08                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281574.36                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283778.12                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.5}\n",
      "loss 282342.59                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281424.98                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 780, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 297850.27                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280177.62                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 285168.96                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 285462.56                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281686.03                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281782.71                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 282481.22                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1570, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 293709.06                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282103.92                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283024.61                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282873.36                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282488.78                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282605.68                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 281412.63                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1750, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 294216.98                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280475.59                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281623.66                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282588.75                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282213.07                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280748.50                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 990, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 284590.88                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1690, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 317002.82                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 285057.66                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 280684.10                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281611.67                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281417.33                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281673.40                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 282058.73                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1610, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 294211.43                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 284077.63                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282009.48                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283363.74                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283399.26                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 870, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282686.86                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.6000000000000001}\n",
      "loss 281437.41                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1400, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 297705.52                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282983.93                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282215.21                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281568.07                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281696.83                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282019.33                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 285380.43                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1040, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 296401.38                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280411.86                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.03, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281972.78                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282608.16                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282233.77                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282516.19                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.7000000000000001}\n",
      "loss 281959.10                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1240, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 298952.56                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 280525.75                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 284183.44                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282096.55                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 284741.35                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282350.29                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 282057.94                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1750, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 296037.64                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283289.25                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 281783.93                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 285808.98                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283132.36                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 900, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282041.58                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 281473.62                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2010, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 295402.52                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283646.45                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 282254.71                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281615.49                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280079.23                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281993.57                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280330.04                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280499.09                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281796.98                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282099.26                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281542.17                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281445.99                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282707.80                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282869.49                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282043.22                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280402.39                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282066.49                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281557.54                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281091.30                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1360, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281372.72                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283229.10                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 281871.38                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280532.02                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281647.74                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281566.48                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283994.56                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1400, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282868.53                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280370.89                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281841.86                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281457.34                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282540.86                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283612.04                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283507.43                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 284035.54                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 281133.32                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282128.68                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.8}\n",
      "loss 281128.74                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281604.73                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1350, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 296168.57                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282188.48                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281549.31                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280309.09                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282061.15                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283444.83                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 281008.28                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282768.23                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1710, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 295696.57                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283368.33                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280322.44                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282727.59                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1360, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281691.91                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282983.93                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 281565.52                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282254.71                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1140, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 296814.91                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284272.73                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282778.15                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281734.04                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282560.48                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280309.09                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 281712.36                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1970, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 294905.23                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282054.59                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 281886.45                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281693.85                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282360.54                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280370.89                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283700.62                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1400, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 282106.16                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282299.96                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1120, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 296573.27                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282146.66                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280221.93                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281821.90                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283424.87                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282350.63                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 285649.43                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 281177.57                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1670, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 297717.21                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 285791.97                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284980.50                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282212.98                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281300.23                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280250.35                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282416.56                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1440, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 282128.68                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 296581.46                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281865.62                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281990.61                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280250.35                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284463.16                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283143.59                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281653.89                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.7000000000000001}\n",
      "loss 281557.93                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1510, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 296536.94                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281209.35                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282828.22                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282543.96                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280313.75                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281822.51                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281920.87                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1360, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 281886.68                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1230, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 294029.46                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 285100.32                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282414.55                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 80, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 288134.78                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.04, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282549.10                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282193.86                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282070.77                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.8}\n",
      "loss 281324.86                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1410, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 293704.60                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281698.78                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281758.50                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280728.46                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284019.84                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281990.61                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283049.53                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 281100.35                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1200, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 296613.64                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281248.30                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281687.47                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283674.52                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282252.56                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282408.49                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282547.96                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 970, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 281612.04                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 1610, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 297806.73                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283594.13                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281641.81                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281541.07                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281121.05                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1300, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281622.06                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280250.33                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 284102.08                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2150, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 310059.72                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281455.52                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282364.16                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282311.64                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281300.28                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 285689.13                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281099.20                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.7000000000000001}\n",
      "loss 282028.02                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281712.36                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1920, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 295747.14                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281646.08                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282084.63                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281241.66                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282336.20                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.09, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281904.03                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 282477.37                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280406.40                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2560, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 297678.03                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281822.16                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281572.15                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 284291.79                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281459.37                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282422.53                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 220, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 283686.92                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281912.74                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282854.26                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2240, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 296381.93                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280691.81                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282793.64                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281854.50                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282057.94                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283344.09                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1530, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 292932.15                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282476.17                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281170.49                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283803.96                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281887.65                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283024.61                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282707.57                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282265.12                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2170, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 293272.17                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 281380.18                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281551.22                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280330.04                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281672.22                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280161.36                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280221.93                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1400, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282106.16                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280161.36                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281549.21                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280395.41                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282029.00                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283365.07                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281323.39                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282685.62                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280327.20                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 281658.91                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281589.08                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281873.91                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284183.44                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281990.61                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282576.43                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280250.35                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281846.76                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282531.92                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281604.73                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281643.14                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1150, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281847.80                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282454.52                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283685.42                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280460.38                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281814.30                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281850.70                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281889.85                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281658.91                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281209.35                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281969.29                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283790.51                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281873.91                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282031.93                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281760.93                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283444.83                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280568.83                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282543.96                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282467.34                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 282223.27                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1550, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 295926.72                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281627.60                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281852.57                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281958.64                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280160.58                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281796.12                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281656.97                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280255.63                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 283298.31                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1750, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 293497.26                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282156.63                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284023.01                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282128.86                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282062.69                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.14, 'n_estimators': 1440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283112.14                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280403.77                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 281886.45                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281591.37                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1840, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 295619.45                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281841.92                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282577.71                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281459.37                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282527.98                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283899.50                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281559.21                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282509.06                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2000, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.7000000000000001}\n",
      "loss 281511.96                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1930, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 295445.82                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281984.53                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283039.22                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282019.33                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282484.31                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 285817.02                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282300.91                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282481.18                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 282573.19                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1690, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 297694.03                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282748.82                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280404.95                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281591.37                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 284631.40                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284180.39                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1400, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281799.24                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 282857.62                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.7000000000000001}\n",
      "loss 281492.19                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1710, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 296661.04                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281962.68                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280168.57                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283695.35                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283024.61                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282498.91                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281854.50                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281490.32                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 283798.66                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1840, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 297668.89                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282440.07                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.09, 'n_estimators': 1440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282128.68                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281840.34                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282540.86                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280161.36                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281417.33                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281397.65                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.6000000000000001}\n",
      "loss 284323.67                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280475.59                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1540, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 296079.95                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282084.63                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282964.69                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 283422.39                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283674.30                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282225.62                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282111.15                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1140, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.7000000000000001}\n",
      "loss 281765.15                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280509.70                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 760, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 298124.69                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281625.20                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283641.60                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282354.20                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280431.30                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282366.79                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1770, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282596.55                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282133.52                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 282587.16                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1540, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 297710.41                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282182.54                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281686.03                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282817.28                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 282329.48                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 280403.77                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 285637.83                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281587.43                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.7000000000000001}\n",
      "loss 281608.85                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1980, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 294909.44                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282160.97                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280221.93                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281822.51                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281946.13                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 283350.75                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282323.66                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281904.03                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 281247.45                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1210, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 298278.57                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 283810.66                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 281712.36                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 283697.48                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 280397.49                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 282540.74                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 281948.33                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280160.58                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 281591.37                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282531.92                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1460, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 296371.54                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282045.62                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282267.81                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282166.24                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280151.92                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282133.52                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281239.25                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 281915.07                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281245.95                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 295373.58                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 285526.22                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283807.55                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281814.42                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281656.92                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281611.67                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282569.62                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 281700.26                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281850.33                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1920, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 312067.88                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283304.65                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281605.49                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280532.02                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 284053.76                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282481.18                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281613.67                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 282054.59                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281912.74                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1660, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 296006.21                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284930.60                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 284143.60                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2150, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280882.51                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280079.23                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282266.33                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281361.97                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280667.99                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.9}\n",
      "loss 280775.04                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1400, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 296070.44                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281643.30                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1910, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281544.58                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280619.09                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282223.51                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281615.49                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281840.34                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282856.96                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 1.0}\n",
      "loss 282312.95                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1780, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301557.35                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1360, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284242.21                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282751.30                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283706.47                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281514.13                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281541.07                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282592.17                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282537.14                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 281524.10                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 420, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 307149.56                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282265.96                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281918.82                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282009.48                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280079.23                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281557.54                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280388.74                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282032.43                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1400, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281799.24                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280079.23                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281995.14                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282576.43                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281937.49                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1770, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280184.82                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281549.21                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282099.26                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280381.57                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282091.41                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281495.81                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.11, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282563.62                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282921.40                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281613.67                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281873.91                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1300, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281622.06                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1360, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280642.08                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281742.32                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281355.84                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280443.00                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282043.22                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281778.12                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284212.76                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282768.87                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1770, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280184.82                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282377.26                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282515.18                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281993.57                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281400.81                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281431.36                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280380.12                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281565.52                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281591.37                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282793.64                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282231.59                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280313.75                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281643.14                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281171.08                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281731.09                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281478.47                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280395.41                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283190.45                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282543.96                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281623.66                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 1.0}\n",
      "loss 281937.49                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281950.08                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1510, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 295693.16                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283134.44                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 284180.39                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281918.82                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282613.24                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281963.96                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280898.22                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281235.41                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.9}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 282268.05                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282563.62                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 293966.87                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283453.73                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281542.42                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282540.86                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282491.04                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 30, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 304087.75                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281351.23                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280509.70                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 1.0}\n",
      "loss 281920.87                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2650, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 297100.14                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280250.33                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282418.31                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282066.02                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282012.92                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281542.17                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283578.10                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281580.59                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281190.93                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 282306.24                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283024.61                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2050, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 297753.85                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281910.99                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280202.92                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281841.92                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281626.03                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281850.88                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1030, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282177.14                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282028.02                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 283803.54                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281495.81                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1700, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 295849.76                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1300, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281186.84                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281615.49                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282410.15                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282420.18                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281574.36                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282251.76                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280316.30                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 1.0}\n",
      "loss 281572.64                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282981.93                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1080, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 297107.12                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282360.54                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.13, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283314.31                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282091.41                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282768.87                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281990.61                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280623.84                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281910.99                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 281342.91                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283317.71                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1150, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 298269.47                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281698.78                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283450.00                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 285356.54                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283997.86                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282054.59                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281623.92                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281963.96                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 1.0}\n",
      "loss 281487.88                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283365.07                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1970, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 293466.45                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282053.68                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282984.58                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281379.11                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284890.01                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282841.85                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280379.14                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281662.41                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 280785.02                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281549.21                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1920, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 293469.24                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281840.34                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283399.26                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280189.72                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282329.48                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 284435.56                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280381.57                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282791.82                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 1.0}\n",
      "loss 282064.75                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281557.54                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1810, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 297871.61                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1530, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281549.21                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1400, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282106.16                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282543.63                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281607.61                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 280406.40                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283315.98                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281758.67                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 281700.26                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281700.85                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1850, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 296556.88                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281990.61                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1770, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281887.71                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280328.87                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283700.62                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282391.76                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281826.58                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282566.06                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 281952.40                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282197.99                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1480, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 298054.26                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280397.72                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281658.91                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 910, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282954.72                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281634.58                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282061.37                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283154.65                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281813.82                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 283134.44                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280327.20                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1610, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 296161.40                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281232.22                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281549.31                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281542.34                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282066.49                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280579.77                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282941.28                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281517.87                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.9}\n",
      "loss 281842.32                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281574.48                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282564.04                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2280, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 295003.13                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 284202.59                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281846.76                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281687.92                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283183.03                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 284707.82                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280160.58                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 283327.16                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'n_estimators': 1070, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.9}\n",
      "loss 281726.29                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1470, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 296545.52                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281703.79                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282403.28                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 280475.59                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281565.52                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283641.60                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282343.97                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283444.83                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281958.64                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 282111.15                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1780, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 296113.65                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281213.66                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281446.26                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 283712.79                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 280079.23                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282594.71                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282644.41                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 282288.00                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 281355.84                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1770, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 281144.42                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1680, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 295858.93                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 282452.59                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281529.92                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 281860.20                                                                  \n",
      "100%|███| 1000/1000 [2:09:37<00:00,  7.78s/trial, best loss: 280079.22661023727]\n"
     ]
    }
   ],
   "source": [
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=1000, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48258afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "1/0\n",
    "with open('trials.pkl', 'wb') as f:\n",
    "    pkl.dump(trials, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d196e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('trials.pkl', 'rb') as f:\n",
    "    trials = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f3514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(doc, pref=''):\n",
    "    res = {}\n",
    "    for k, v in doc.items():\n",
    "        k = f'{pref}.{k}' if pref else k\n",
    "        if isinstance(v, dict):\n",
    "            res.update(flatten(v, k))\n",
    "        else:\n",
    "            res[k] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "819a28ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>tr_loss</th>\n",
       "      <th>params.learning_rate</th>\n",
       "      <th>params.n_estimators</th>\n",
       "      <th>params.objective</th>\n",
       "      <th>params.reg_alpha</th>\n",
       "      <th>params.subsample</th>\n",
       "      <th>train_time</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>280079.22661</td>\n",
       "      <td>253562.038765</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1550</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.387196</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>280079.22661</td>\n",
       "      <td>253562.038765</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1550</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.414878</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>280079.22661</td>\n",
       "      <td>253562.038765</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1550</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.387109</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>280079.22661</td>\n",
       "      <td>253562.038765</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1550</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.359970</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>280079.22661</td>\n",
       "      <td>253562.038765</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1550</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.531488</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss        tr_loss  params.learning_rate  params.n_estimators  \\\n",
       "990  280079.22661  253562.038765                  0.08                 1550   \n",
       "402  280079.22661  253562.038765                  0.08                 1550   \n",
       "791  280079.22661  253562.038765                  0.08                 1550   \n",
       "759  280079.22661  253562.038765                  0.08                 1550   \n",
       "786  280079.22661  253562.038765                  0.08                 1550   \n",
       "\n",
       "    params.objective  params.reg_alpha  params.subsample  train_time status  \n",
       "990       regression               0.0               0.9    3.387196     ok  \n",
       "402       regression               0.0               0.7    3.414878     ok  \n",
       "791       regression               0.0               0.9    3.387109     ok  \n",
       "759       regression               0.0               0.9    3.359970     ok  \n",
       "786       regression               0.0               0.9    3.531488     ok  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(map(flatten, [e['result'] for e in trials.trials])))\n",
    "df.sort_values('loss').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eab8880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.loss.plot()\n",
    "plt.scatter([df.loss.argmin()], [df.loss.min()], c='r')\n",
    "# df.tr_loss.plot()\n",
    "plt.yscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8896249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df.loss.rolling(20).mean().plot()\n",
    "plt.scatter([df.loss.argmin()], [df.loss.min()], c='r')\n",
    "df.tr_loss.rolling(20).mean().plot()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2a32093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df.tr_loss, df.loss, c=(df.loss-df.tr_loss)/df.loss*100)\n",
    "plt.title('Comparison of training and dev losses.\\n Color corresponds to overfitting percentage')\n",
    "plt.colorbar()\n",
    "m = min(df.tr_loss.min(), df.loss.min())\n",
    "M = max(df.tr_loss.max(), df.loss.max())\n",
    "plt.plot([m, M], [m, M], 'k--')\n",
    "plt.xlabel('tr loss')\n",
    "plt.ylabel('dev loss')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d908644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>tr_loss</th>\n",
       "      <th>params.learning_rate</th>\n",
       "      <th>params.n_estimators</th>\n",
       "      <th>params.objective</th>\n",
       "      <th>params.reg_alpha</th>\n",
       "      <th>params.subsample</th>\n",
       "      <th>train_time</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>299221.864446</td>\n",
       "      <td>235478.103361</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1460</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.260258</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>299083.208546</td>\n",
       "      <td>233732.075608</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1610</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.511403</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>299151.785484</td>\n",
       "      <td>233138.195638</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1670</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.728529</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>299161.054625</td>\n",
       "      <td>232546.925179</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1730</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.837287</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>299161.054625</td>\n",
       "      <td>232546.925179</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1730</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.621785</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>299183.581488</td>\n",
       "      <td>232309.084073</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1920</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.071576</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>299102.303181</td>\n",
       "      <td>232090.719539</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1780</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.647600</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>299168.587297</td>\n",
       "      <td>231548.172805</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2000</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.122056</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>299183.105988</td>\n",
       "      <td>231363.701008</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2020</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.9</td>\n",
       "      <td>7.445719</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>299113.721006</td>\n",
       "      <td>231260.397753</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2030</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.143197</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>299127.468663</td>\n",
       "      <td>231202.561658</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2040</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.204910</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>299103.924489</td>\n",
       "      <td>231130.514778</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2050</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.234586</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>299127.127667</td>\n",
       "      <td>230504.145538</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2130</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.340780</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>299124.674781</td>\n",
       "      <td>230447.609101</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2140</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.181825</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>299217.376215</td>\n",
       "      <td>230318.961245</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1940</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.227202</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>299085.649977</td>\n",
       "      <td>230300.867520</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2160</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.346664</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>299087.553423</td>\n",
       "      <td>230196.654260</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.380321</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>299042.412997</td>\n",
       "      <td>230158.005516</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2020</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.070480</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>298997.261117</td>\n",
       "      <td>229913.494211</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2210</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.441961</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>298999.506587</td>\n",
       "      <td>229885.888483</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2060</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.940384</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>299003.323929</td>\n",
       "      <td>229826.983574</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2070</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.039256</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>299075.225318</td>\n",
       "      <td>229752.642758</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2230</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.456400</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>299056.030698</td>\n",
       "      <td>229465.406707</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2260</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.463450</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>298962.620426</td>\n",
       "      <td>229039.443816</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.884760</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>298962.620426</td>\n",
       "      <td>229039.443816</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.703802</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>298962.620421</td>\n",
       "      <td>229039.443760</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.433737</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>298962.620429</td>\n",
       "      <td>229039.443709</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.487250</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>298969.271321</td>\n",
       "      <td>228985.748815</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2320</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.579816</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>298969.271321</td>\n",
       "      <td>228985.748815</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2320</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.083797</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>299041.961269</td>\n",
       "      <td>228942.422155</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.399201</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss        tr_loss  params.learning_rate  params.n_estimators  \\\n",
       "77   299221.864446  235478.103361                  0.14                 1460   \n",
       "289  299083.208546  233732.075608                  0.14                 1610   \n",
       "117  299151.785484  233138.195638                  0.14                 1670   \n",
       "147  299161.054625  232546.925179                  0.14                 1730   \n",
       "257  299161.054625  232546.925179                  0.14                 1730   \n",
       "500  299183.581488  232309.084073                  0.13                 1920   \n",
       "73   299102.303181  232090.719539                  0.14                 1780   \n",
       "712  299168.587297  231548.172805                  0.13                 2000   \n",
       "559  299183.105988  231363.701008                  0.13                 2020   \n",
       "985  299113.721006  231260.397753                  0.13                 2030   \n",
       "920  299127.468663  231202.561658                  0.13                 2040   \n",
       "680  299103.924489  231130.514778                  0.13                 2050   \n",
       "804  299127.127667  230504.145538                  0.13                 2130   \n",
       "694  299124.674781  230447.609101                  0.13                 2140   \n",
       "548  299217.376215  230318.961245                  0.14                 1940   \n",
       "778  299085.649977  230300.867520                  0.13                 2160   \n",
       "510  299087.553423  230196.654260                  0.13                 2170   \n",
       "68   299042.412997  230158.005516                  0.14                 2020   \n",
       "610  298997.261117  229913.494211                  0.13                 2210   \n",
       "351  298999.506587  229885.888483                  0.14                 2060   \n",
       "327  299003.323929  229826.983574                  0.14                 2070   \n",
       "597  299075.225318  229752.642758                  0.13                 2230   \n",
       "797  299056.030698  229465.406707                  0.13                 2260   \n",
       "746  298962.620426  229039.443816                  0.13                 2310   \n",
       "907  298962.620426  229039.443816                  0.13                 2310   \n",
       "914  298962.620421  229039.443760                  0.13                 2310   \n",
       "822  298962.620429  229039.443709                  0.13                 2310   \n",
       "468  298969.271321  228985.748815                  0.13                 2320   \n",
       "859  298969.271321  228985.748815                  0.13                 2320   \n",
       "314  299041.961269  228942.422155                  0.14                 2170   \n",
       "\n",
       "    params.objective  params.reg_alpha  params.subsample  train_time status  \n",
       "77        regression              0.00               1.0    3.260258     ok  \n",
       "289       regression              0.00               1.0    3.511403     ok  \n",
       "117       regression              0.00               1.0    3.728529     ok  \n",
       "147       regression              0.00               1.0    3.837287     ok  \n",
       "257       regression              0.00               1.0    3.621785     ok  \n",
       "500       regression              0.05               0.8    4.071576     ok  \n",
       "73        regression              0.00               1.0    3.647600     ok  \n",
       "712       regression              0.05               0.8    4.122056     ok  \n",
       "559       regression              0.06               0.9    7.445719     ok  \n",
       "985       regression              0.03               0.7    4.143197     ok  \n",
       "920       regression              0.05               0.7    4.204910     ok  \n",
       "680       regression              0.05               0.7    4.234586     ok  \n",
       "804       regression              0.04               0.8    4.340780     ok  \n",
       "694       regression              0.05               0.7    4.181825     ok  \n",
       "548       regression              0.02               0.8    4.227202     ok  \n",
       "778       regression              0.05               0.9    4.346664     ok  \n",
       "510       regression              0.03               0.8    5.380321     ok  \n",
       "68        regression              0.00               0.9    5.070480     ok  \n",
       "610       regression              0.03               0.7    4.441961     ok  \n",
       "351       regression              0.00               0.8    4.940384     ok  \n",
       "327       regression              0.00               0.8    4.039256     ok  \n",
       "597       regression              0.05               0.8    4.456400     ok  \n",
       "797       regression              0.06               0.8    4.463450     ok  \n",
       "746       regression              0.05               0.9    4.884760     ok  \n",
       "907       regression              0.05               0.7    4.703802     ok  \n",
       "914       regression              0.04               0.7    4.433737     ok  \n",
       "822       regression              0.03               0.8    4.487250     ok  \n",
       "468       regression              0.04               0.8    4.579816     ok  \n",
       "859       regression              0.04               0.7    5.083797     ok  \n",
       "314       regression              0.00               0.8    5.399201     ok  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = df[df.loss < df.loss.min() * 1.001].sort_values('tr_loss', ascending=False).head(30)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68d2a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42921e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tr_loss': 243758.34842441877,\n",
       " 'dev_loss': 283105.2058477523,\n",
       " 'test_loss': 228476.032745023}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {k.replace('params.', ''):v for k, v in best.iloc[0].to_dict().items() if 'params.' in k}\n",
    "rf_pipe = make_pipeline(\n",
    "    features_pipe,\n",
    "    lgb.LGBMRegressor(random_state=42, **params)\n",
    ")\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "best_model_performance = dict(\n",
    "    tr_loss=rmse(y_train, rf_pipe.predict(X_train)), \n",
    "    dev_loss=rmse(y_dev, rf_pipe.predict(X_dev)),\n",
    "    test_loss=rmse(y_test, rf_pipe.predict(X_test)), \n",
    ")\n",
    "best_model_performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
