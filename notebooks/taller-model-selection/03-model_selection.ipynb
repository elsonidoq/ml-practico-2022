{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdeac8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: numpy in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (1.23.0)\n",
      "Requirement already satisfied: scipy in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (1.8.1)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/przivic/miniconda3/envs/mlp2022/lib/python3.9/site-packages (from hyperopt) (4.64.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.2\n",
      "  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Using cached cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ac71af753f3c1160c6a0b3f5007c35795d87af60b0b217ad8b4d6a86beacf5a8\n",
      "  Stored in directory: /Users/przivic/Library/Caches/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "Successfully built future\n",
      "Installing collected packages: py4j, networkx, future, cloudpickle, hyperopt\n",
      "Successfully installed cloudpickle-2.1.0 future-0.18.2 hyperopt-0.2.7 networkx-2.8.4 py4j-0.10.9.5\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e5be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-3.3.2-py3-none-macosx_10_14_x86_64.macosx_10_15_x86_64.macosx_11_0_x86_64.whl (1.2 MB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.23.1-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.1 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Using cached scipy-1.8.1-cp39-cp39-macosx_12_0_universal2.macosx_10_9_x86_64.whl (55.6 MB)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92093c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16976a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taller_model_selection.serialize import iter_jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ecd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = map(list, map(iter_jl, ['X_train.jl', 'y_train.jl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6b9eb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121272, 121272, 20212, 20212, 20212, 20212)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "len(X_train), len(y_train), len(X_dev), len(y_dev), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13b98763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taller_model_selection.metrics import rmse\n",
    "from taller_model_selection.transformers import FeatureProjection\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "features_pipe = make_union(\n",
    "    make_pipeline(\n",
    "        FeatureProjection(['rooms', 'bedrooms', 'bathrooms', 'surface_total', 'surface_covered']),\n",
    "        SimpleImputer()\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        FeatureProjection(['l3']), \n",
    "        SimpleImputer(strategy='most_frequent'),\n",
    "        OneHotEncoder(sparse=False)\n",
    "    ), \n",
    "    make_pipeline(\n",
    "        FeatureProjection(['l4']), \n",
    "        SimpleImputer(strategy='constant'),\n",
    "        OneHotEncoder(sparse=False)\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba580eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from pprint import pprint\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from time import time\n",
    "\n",
    "def eval_pipe(model_name, pipe):\n",
    "    return dict(\n",
    "        name=model_name,\n",
    "        train=rmse(y_train, pipe.predict(X_train)),\n",
    "        dev=rmse(y_dev, pipe.predict(X_dev))\n",
    "    )\n",
    "\n",
    "def rf_objective(params):\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    print(params)\n",
    "    rf_pipe = make_pipeline(\n",
    "        features_pipe,\n",
    "        lgb.LGBMRegressor(random_state=42, **params)\n",
    "    )\n",
    "    t0 = time()\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    loss=rmse(y_dev, rf_pipe.predict(X_dev))\n",
    "    print(f'loss {loss:.02f}')\n",
    "    return dict(\n",
    "        loss=loss,\n",
    "        tr_loss=rmse(y_train, rf_pipe.predict(X_train)), \n",
    "        params=params,\n",
    "        train_time=train_time,\n",
    "        status=STATUS_OK\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d9c804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 3000, 10),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1.0, 0.1),\n",
    "    'objective': hp.choice('objective', [\"regression\", \"regression_l1\"]),\n",
    "    'learning_rate': hp.qloguniform('learning_rate', np.log(0.01), np.log(0.3), 0.01),\n",
    "    'reg_alpha': hp.choice('ra', [0, hp.quniform('reg_alpha', 0.01, 0.1, 0.01)]),\n",
    "}\n",
    "\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1cd5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 2580, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.8}\n",
      "loss 308031.03                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2860, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.7000000000000001}\n",
      "loss 307555.46                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1500, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 328279.82                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 350, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 310182.88                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 600, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 336534.25                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1520, 'objective': 'regression_l1', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 329366.82                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1770, 'objective': 'regression_l1', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 337593.22                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 670, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 334469.65                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 910, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 330485.31                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 420, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.7000000000000001}\n",
      "loss 313554.64                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1200, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 328279.82                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 2750, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 328263.21                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2050, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 330344.16                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 309265.62                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 360, 'objective': 'regression_l1', 'reg_alpha': 0.09, 'subsample': 0.6000000000000001}\n",
      "loss 360536.75                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 304461.93                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2760, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.5}\n",
      "loss 299868.46                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 820, 'objective': 'regression_l1', 'reg_alpha': 0.1, 'subsample': 0.6000000000000001}\n",
      "loss 334807.49                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 315381.19                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 950, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 332456.91                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 40, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 314507.54                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299771.98                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300478.00                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 300034.30                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300843.10                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 300113.29                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 300014.67                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 299936.20                                                                  \n",
      "{'learning_rate': 0.29, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 1.0}\n",
      "loss 299724.31                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299890.83                                                                  \n",
      "{'learning_rate': 0.29, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 299607.48                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 302075.20                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 305921.49                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 1.0}\n",
      "loss 300959.55                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 301149.74                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 1.0}\n",
      "loss 299960.63                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 301526.40                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 303357.33                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299873.66                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 300888.91                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 299696.56                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1040, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 300741.90                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.06, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300934.31                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2170, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 326414.81                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 80, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 311422.49                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1180, 'objective': 'regression_l1', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 329573.47                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 300650.09                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1430, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 328019.02                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299853.84                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 301991.14                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 770, 'objective': 'regression_l1', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 327942.47                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1050, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.6000000000000001}\n",
      "loss 306680.41                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1510, 'objective': 'regression_l1', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 335859.10                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2840, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299920.09                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300075.94                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 510, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 317605.36                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2260, 'objective': 'regression_l1', 'reg_alpha': 0.08, 'subsample': 0.6000000000000001}\n",
      "loss 334059.93                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 309013.42                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2540, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.5}\n",
      "loss 300311.98                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 1830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 301289.05                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 2680, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 331529.01                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 300721.22                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299478.95                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301885.01                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1120, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 351631.10                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300700.21                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 900, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 304010.71                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299723.04                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299042.41                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301138.33                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300979.85                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 302862.54                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2150, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300115.01                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299102.30                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300209.87                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 304191.40                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300234.70                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1460, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299221.86                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301573.59                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300790.34                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301208.25                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299771.98                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1750, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 331395.14                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301499.86                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.14, 'n_estimators': 690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301324.23                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300083.14                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301839.00                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2220, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 327206.82                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 302206.78                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299915.74                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2900, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300790.98                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1460, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 338354.57                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300416.01                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301565.07                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 570, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 301795.98                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 304396.82                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1390, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 330391.25                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300594.63                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 308568.54                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 301232.66                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2090, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 332103.21                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300584.12                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299538.19                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 300232.06                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299407.97                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1700, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 330750.44                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300737.90                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299726.72                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300121.66                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300636.95                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 1580, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 335546.52                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300263.41                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300569.64                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301907.45                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 303386.80                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 304862.41                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2560, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 325752.94                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299151.79                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300413.93                                                                  \n",
      "{'learning_rate': 0.29, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299151.41                                                                  \n",
      "{'learning_rate': 0.29, 'n_estimators': 2840, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300790.94                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 450, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 376566.07                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301794.84                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 300518.59                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 310116.61                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 1220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301667.23                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 1790, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 327766.69                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.17, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299926.82                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300585.98                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299629.63                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299395.84                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 620, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 335289.27                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.9}\n",
      "loss 300112.66                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 302790.24                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 301177.24                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300519.84                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2700, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 328279.82                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 140, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 326267.42                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300098.82                                                                  \n",
      "{'learning_rate': 0.29, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300496.29                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 302144.83                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 950, 'objective': 'regression_l1', 'reg_alpha': 0.1, 'subsample': 1.0}\n",
      "loss 332697.16                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 308634.13                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301791.86                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300328.72                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300220.22                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300209.57                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299161.05                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300287.61                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299631.72                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300413.37                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301319.56                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300144.23                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299370.99                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 2510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299956.73                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300008.80                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300806.22                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301537.00                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299737.53                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299659.79                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299819.84                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301446.28                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1150, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 301426.84                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1560, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 330061.58                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 305612.76                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299697.12                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300910.72                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299632.21                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 1.0}\n",
      "loss 301548.78                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 870, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 330456.79                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.3, 'n_estimators': 700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301769.30                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 299579.85                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300039.11                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.8}\n",
      "loss 301134.94                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2450, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 327131.53                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300692.83                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299880.49                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300376.67                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 303757.12                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1720, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 304917.15                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2260, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 325829.58                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 305659.56                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299309.40                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300725.24                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300245.86                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.8}\n",
      "loss 300203.74                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2590, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 329643.33                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301290.56                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300491.87                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 770, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 302844.24                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301868.86                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 310087.04                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2740, 'objective': 'regression_l1', 'reg_alpha': 0.1, 'subsample': 0.5}\n",
      "loss 331570.76                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300373.96                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 302627.20                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 301412.81                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1340, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299375.14                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 301200.03                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 340, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 344062.73                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299996.25                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299577.90                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300552.42                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300057.28                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 1.0}\n",
      "loss 300533.83                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1660, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 330061.58                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 301062.27                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299180.60                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 302232.01                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301370.70                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 300433.38                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 331213.33                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1310, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300899.96                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300699.61                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.06, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300301.68                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 305332.95                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300812.21                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1840, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 326168.46                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301748.84                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299755.46                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300300.76                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 303864.20                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1050, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 1.0}\n",
      "loss 301496.06                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 1890, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 329431.16                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 303047.73                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 1110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301287.01                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300277.47                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300531.01                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 301251.63                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 335793.84                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299352.39                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 305125.69                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301228.83                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300345.83                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299321.60                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1950, 'objective': 'regression_l1', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 341332.75                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300162.57                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299471.22                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300204.30                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 303909.07                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 302232.76                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1680, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 329995.29                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 302088.41                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300071.96                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300799.87                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 305481.43                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301560.59                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1840, 'objective': 'regression_l1', 'reg_alpha': 0.09, 'subsample': 1.0}\n",
      "loss 323670.75                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300708.10                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 303721.07                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301516.76                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299694.73                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 300562.12                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.7000000000000001}\n",
      "loss 300154.11                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1200, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 327806.77                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 40, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 315259.17                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 303255.59                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 1040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301786.80                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299161.05                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299344.63                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299895.42                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300272.31                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299249.18                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300146.39                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300003.00                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299666.96                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299610.77                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300232.03                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301238.32                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300771.55                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299926.82                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 1410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301771.09                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 1670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301119.21                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299462.66                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299345.68                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301009.83                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301256.59                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1580, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300391.31                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299740.23                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301508.36                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299880.22                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301283.75                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1330, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 329781.09                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299831.96                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301775.18                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2150, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299682.49                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300727.67                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1680, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299443.10                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 299812.70                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1370, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 330406.88                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1610, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299083.21                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300306.36                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 1190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300154.88                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301790.02                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300066.49                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1810, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 300750.30                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1540, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 331316.12                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1640, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300078.56                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 920, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 302099.56                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 301742.15                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 300340.69                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1150, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 311714.52                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 770, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 310031.35                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2020, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 332103.21                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 302064.53                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300509.01                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300578.21                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299778.66                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 303563.09                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1300, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 300664.98                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1780, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 331466.73                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 1590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 299662.21                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2110, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 299625.09                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 298966.53                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300121.66                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2170, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299041.96                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2690, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299325.09                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2770, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299223.49                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2400, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 330285.44                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299762.47                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2880, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299507.44                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299836.37                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300185.89                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2540, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300052.88                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.8}\n",
      "loss 299687.71                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2180, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 328694.52                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299406.67                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300670.70                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299003.32                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300014.67                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2470, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299983.28                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 3000, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299517.67                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2580, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299306.02                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2360, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 328279.82                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299823.28                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300921.65                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299986.98                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300205.75                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2510, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299459.32                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2730, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299126.87                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2360, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 329878.59                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300929.24                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2820, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299618.24                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.11, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300106.72                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300781.73                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 305006.93                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2170, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299940.57                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2640, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 328716.02                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300033.51                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300307.92                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299775.53                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300047.35                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 298999.51                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 299590.02                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2450, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 330262.37                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299490.52                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2700, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299402.82                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2060, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300154.15                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299809.60                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299632.21                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 300090.89                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2500, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 327975.92                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2860, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300357.50                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299100.49                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300101.46                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 300521.53                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299720.50                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 300619.14                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1910, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 328233.31                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2150, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299832.33                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300114.50                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2980, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299521.94                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300083.14                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2910, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299349.44                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299303.42                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2790, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300089.95                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1980, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 330061.58                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299445.76                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299073.56                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 301928.64                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301853.31                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299751.27                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 301479.34                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2050, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 332103.21                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1910, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 300042.41                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 308172.94                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300162.94                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1780, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300000.23                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 2720, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.7000000000000001}\n",
      "loss 301950.08                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2620, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300556.50                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1960, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 331447.75                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299641.18                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299712.50                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 300229.13                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300943.19                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 303489.87                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300062.26                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2530, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 330391.25                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2190, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299263.67                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300697.98                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 308315.39                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 301410.48                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 299104.55                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300074.12                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 299286.37                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2590, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299870.90                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299852.00                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300105.91                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2750, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 299170.57                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299579.85                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300767.38                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299850.28                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 300100.36                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299502.83                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299394.60                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299834.51                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 300014.17                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2550, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299363.41                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300626.82                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299115.60                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300919.00                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300029.67                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299902.15                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300006.69                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.5}\n",
      "loss 300151.27                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299688.35                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2630, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 299355.67                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299042.38                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299825.80                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 301370.70                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300275.34                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299788.82                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.8}\n",
      "loss 299922.86                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2190, 'objective': 'regression_l1', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 330562.00                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1730, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299479.68                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300086.66                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2240, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300656.76                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 300323.11                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299532.54                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.8}\n",
      "loss 299850.36                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 299013.78                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 2540, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 301398.65                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2740, 'objective': 'regression_l1', 'reg_alpha': 0.1, 'subsample': 0.8}\n",
      "loss 327367.30                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 298995.07                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2840, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299649.83                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299026.91                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2660, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299070.84                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 3000, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299520.04                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2890, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 300461.15                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2810, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299899.05                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2710, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 331905.81                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2950, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299719.95                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2580, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 299730.64                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2780, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299621.97                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2490, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299837.69                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300338.19                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2630, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299971.21                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2900, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 299441.34                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2700, 'objective': 'regression_l1', 'reg_alpha': 0.08, 'subsample': 0.8}\n",
      "loss 328305.51                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300153.58                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 298981.22                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299916.36                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300771.05                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299901.51                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299686.09                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299806.67                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2430, 'objective': 'regression_l1', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 329599.28                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 299677.32                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2760, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 299495.11                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 298969.27                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.5}\n",
      "loss 300993.52                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300525.52                                                                  \n",
      "{'learning_rate': 0.29, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300621.10                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299660.61                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2150, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 328629.16                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.5}\n",
      "loss 300149.74                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299919.08                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300087.58                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 305503.39                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2790, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299192.56                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299916.50                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 303408.69                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 3000, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 328975.38                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299792.02                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300070.91                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300353.21                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 2300, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299676.96                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299989.69                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300756.33                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299465.59                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2840, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 327333.49                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 300534.68                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2720, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299610.64                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.5}\n",
      "loss 300082.86                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299709.34                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2470, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299158.95                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299757.73                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1830, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300959.15                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1980, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 300034.35                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1670, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 330701.15                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.6000000000000001}\n",
      "loss 301170.37                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299183.58                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2570, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 300645.87                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299813.20                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2640, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299434.56                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2460, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299657.52                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 301088.93                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1790, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.6000000000000001}\n",
      "loss 332829.36                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299965.16                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2240, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 300042.99                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2880, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299388.17                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2170, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 299087.55                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2510, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.7000000000000001}\n",
      "loss 300545.43                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 300007.47                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299698.35                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1880, 'objective': 'regression_l1', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 330885.50                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2710, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299940.23                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299828.62                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2780, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 299653.24                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 301571.35                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301297.34                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.5}\n",
      "loss 301437.59                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 309244.31                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2240, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 328939.36                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300685.51                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 2920, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 301877.65                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299162.28                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300353.21                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 300791.20                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 299864.30                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 310, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 304159.62                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1780, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 329712.26                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2540, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300533.35                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 299507.89                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.6000000000000001}\n",
      "loss 301625.28                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 3000, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299832.89                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299855.96                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 1900, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 304531.51                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 299446.13                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2820, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 326620.17                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300680.24                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2630, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300731.33                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2300, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299924.04                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.5}\n",
      "loss 299583.77                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2750, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 299874.29                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300124.59                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301071.83                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2570, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 326731.67                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299456.69                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299217.38                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1710, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300110.06                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "loss 300566.76                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 300100.13                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 299831.30                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 1650, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 300975.71                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2110, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 332234.44                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2950, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.7000000000000001}\n",
      "loss 299885.66                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 200, 'objective': 'regression', 'reg_alpha': 0.08, 'subsample': 0.6000000000000001}\n",
      "loss 306553.75                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2700, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300020.61                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299608.91                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299183.11                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1770, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 301704.50                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2550, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299263.00                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2460, 'objective': 'regression_l1', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 328529.92                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299846.69                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299737.59                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2630, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299649.02                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2240, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300120.82                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 2850, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.6000000000000001}\n",
      "loss 301233.91                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 300123.69                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1940, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 299895.97                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2160, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 334608.27                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2490, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299006.56                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299836.24                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2740, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299700.55                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299344.57                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1540, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 300444.74                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 299709.37                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 298995.69                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2590, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300486.45                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 298923.72                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2680, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299495.17                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299863.21                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2550, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299041.55                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2630, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299649.02                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299817.21                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2810, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299622.87                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299474.34                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299853.71                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2730, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299126.87                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300557.64                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2550, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299650.83                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2510, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299855.36                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299291.27                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300016.66                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 299136.99                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299628.91                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299570.47                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299075.23                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2780, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299608.15                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300749.29                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2890, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300312.61                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299421.31                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2570, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300574.57                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299789.25                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300073.99                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2700, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299606.21                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299464.71                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2640, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299931.47                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299867.16                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2510, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299434.50                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 298997.26                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2930, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 299370.98                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 3000, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299520.04                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2790, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300452.32                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299667.40                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 299916.50                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2880, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 327966.02                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2740, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299165.42                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300389.43                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2150, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299208.86                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2530, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.6000000000000001}\n",
      "loss 299713.85                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300945.02                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2830, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299607.56                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300201.89                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2480, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 328369.43                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.6000000000000001}\n",
      "loss 300193.38                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2640, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299931.47                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 299516.93                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2690, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 300418.65                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299885.37                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299085.92                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300035.61                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300491.31                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2460, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 329009.26                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 298923.72                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 2770, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299596.83                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2590, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.6000000000000001}\n",
      "loss 300022.23                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.5}\n",
      "loss 299082.08                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2980, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 299630.88                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2840, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299599.01                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299901.51                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299937.53                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2730, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 326723.96                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299560.03                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299925.70                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 299464.71                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2920, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299431.44                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299777.93                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 301108.96                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 301207.25                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2670, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299570.47                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 308584.37                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2100, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 326427.53                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2800, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.5}\n",
      "loss 299888.86                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 90, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 310376.25                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299881.43                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300443.45                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 298989.50                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2960, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299719.41                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2740, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 300515.67                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2890, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 326513.42                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2830, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299878.40                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299977.46                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2590, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.6000000000000001}\n",
      "loss 299687.50                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2700, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299892.04                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299813.59                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2580, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 302021.97                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 480, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 303286.58                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299730.99                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2770, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299798.07                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2870, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 327932.09                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299863.37                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 300243.88                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300120.96                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299437.97                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2620, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300621.16                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.14, 'n_estimators': 2950, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299455.72                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2530, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299678.37                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300008.08                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2710, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 329103.54                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299103.92                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2190, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299843.14                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2790, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300162.19                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2460, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299719.80                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2110, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299284.73                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 305863.09                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2660, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 301378.86                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 303734.97                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2410, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 327245.31                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2540, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299423.39                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300545.02                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2830, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299608.91                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299574.50                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2740, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 307798.93                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299124.67                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2620, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 299790.41                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299886.42                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 2060, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.5}\n",
      "loss 334282.46                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2900, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 299684.49                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299937.38                                                                  \n",
      "{'learning_rate': 0.27, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.7000000000000001}\n",
      "loss 300363.67                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299856.50                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2990, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299557.97                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299614.23                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2300, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300082.89                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2690, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300054.37                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1950, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 328204.27                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2510, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299191.13                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299716.34                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 301069.89                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299679.57                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299696.88                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2000, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299168.59                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299656.16                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2770, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299877.30                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2240, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 326005.04                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299880.58                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 3000, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.6000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 301415.99                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 298977.80                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2110, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299284.73                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.9}\n",
      "loss 301247.11                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 299907.13                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2850, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 300269.76                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300125.74                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 2570, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 331132.22                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299783.72                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300944.87                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 301467.21                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2710, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.6000000000000001}\n",
      "loss 299594.13                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299437.97                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 300118.37                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299697.88                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2170, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.8}\n",
      "loss 299188.87                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2360, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 329214.86                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299678.72                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2580, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299436.18                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.9}\n",
      "loss 301408.65                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 390, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 304461.93                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2910, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299865.11                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299715.95                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2740, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299610.66                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2240, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299891.20                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2800, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 329082.72                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 2470, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 301554.48                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 300623.98                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.5}\n",
      "loss 299829.72                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 298962.62                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 299344.57                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 300098.05                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1910, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 301390.63                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2110, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300039.03                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2230, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 327381.66                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299850.21                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 299652.56                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1860, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300378.11                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 590, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 306863.57                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299744.07                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 300614.76                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1760, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 1.0}\n",
      "loss 299353.20                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 299849.38                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2360, 'objective': 'regression_l1', 'reg_alpha': 0.08, 'subsample': 0.9}\n",
      "loss 328592.45                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1930, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 1.0}\n",
      "loss 300105.44                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 2190, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 305377.00                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299863.20                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 300944.87                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2050, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299693.04                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 299085.92                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1820, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 1.0}\n",
      "loss 300029.67                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299844.32                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 301238.69                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 1.0}\n",
      "loss 300373.44                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2450, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 326005.04                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 300000.69                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2530, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 299678.37                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 2250, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 303735.38                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299301.90                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 299615.87                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1690, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 299441.05                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299085.65                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2500, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 328355.09                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2300, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.9}\n",
      "loss 299883.91                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 1.0}\n",
      "loss 299621.68                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.9}\n",
      "loss 299997.58                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 300155.40                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2550, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299836.53                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299507.56                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299027.39                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2530, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299258.84                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 298977.80                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299002.03                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2240, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299619.05                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300006.92                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 300171.92                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299098.90                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299853.16                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2300, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299691.78                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299940.56                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299056.03                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299996.87                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2350, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300584.24                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299460.49                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2200, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299714.16                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 299162.28                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299720.10                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299127.13                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299416.74                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299862.39                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299854.86                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299937.38                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300017.46                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299789.18                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2630, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299020.33                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2490, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300163.39                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299264.36                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300427.96                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2680, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.8}\n",
      "loss 299898.53                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299416.74                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2020, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299633.56                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2250, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 299852.17                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299085.92                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2460, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299488.34                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.8}\n",
      "loss 300480.01                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 298962.62                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299728.29                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 300018.93                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299507.20                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 301325.39                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299270.11                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2300, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299691.78                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 299466.14                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299901.62                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 300005.90                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2360, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 326244.97                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2230, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299733.49                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299580.01                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 299057.56                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2420, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.9}\n",
      "loss 299914.47                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 301256.18                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299910.55                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2600, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 300514.58                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 299064.83                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 300254.04                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2060, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 328355.09                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 299823.28                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 299686.09                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 298987.40                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 1880, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.9}\n",
      "loss 299503.73                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300993.52                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 299717.52                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 299612.06                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2680, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.8}\n",
      "loss 299833.24                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 308537.87                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2160, 'objective': 'regression_l1', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 328791.59                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 301602.56                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2370, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300357.31                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1840, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.8}\n",
      "loss 300021.05                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1960, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.8}\n",
      "loss 301309.84                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2580, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 299436.18                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.9}\n",
      "loss 299715.43                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 298969.27                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300031.22                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 299479.51                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299592.69                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1740, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 328667.58                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299293.48                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299699.41                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 299864.30                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1910, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299987.69                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2190, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299638.42                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2640, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299942.95                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300258.85                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 300201.89                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2230, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 328543.68                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 300377.58                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300123.42                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2120, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 299876.11                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2730, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299123.75                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2270, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300016.66                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 10, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 349556.75                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2520, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299412.80                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1620, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301455.17                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.7000000000000001}\n",
      "loss 299229.31                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2320, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 327365.42                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 830, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 302622.01                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300090.78                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.02, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 304764.46                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2410, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300546.73                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299915.67                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.6000000000000001}\n",
      "loss 299809.96                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2260, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300018.44                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299474.34                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2160, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299871.38                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2710, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 328833.79                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2550, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300652.16                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 730, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 301074.87                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300090.85                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 299646.67                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300049.94                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299902.15                                                                  \n",
      "{'learning_rate': 0.25, 'n_estimators': 2660, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 301186.10                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.6000000000000001}\n",
      "loss 300110.90                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2250, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299991.48                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2160, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 327944.45                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300319.64                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 2590, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 308025.95                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2530, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 299713.85                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299293.03                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 298962.62                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 1970, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.6000000000000001}\n",
      "loss 299764.40                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300017.95                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1740, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299932.63                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.6000000000000001}\n",
      "loss 301345.67                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2240, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 326355.76                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2190, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 299873.66                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2310, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 298962.62                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300248.45                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2070, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300005.42                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2460, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 299488.34                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300462.13                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2150, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301253.49                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299127.47                                                                  \n",
      "{'learning_rate': 0.24, 'n_estimators': 2220, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299500.65                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1870, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300094.46                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2400, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 333172.55                                                                  \n",
      "{'learning_rate': 0.28, 'n_estimators': 2490, 'objective': 'regression', 'reg_alpha': 0.09, 'subsample': 0.7000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 301806.74                                                                  \n",
      "{'learning_rate': 0.04, 'n_estimators': 1990, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 302266.55                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 1660, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300272.15                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2290, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299864.30                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 299822.30                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299812.93                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2330, 'objective': 'regression', 'reg_alpha': 0.1, 'subsample': 0.7000000000000001}\n",
      "loss 299745.16                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299724.48                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2430, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299327.62                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2740, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 327966.02                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2530, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299889.94                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2650, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 301508.46                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2250, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299746.73                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2790, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299631.32                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2040, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301105.38                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2360, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.6000000000000001}\n",
      "loss 300064.89                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 2480, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300082.08                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299397.46                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 1800, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 300517.85                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2170, 'objective': 'regression_l1', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 326005.04                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2560, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299596.69                                                                  \n",
      "{'learning_rate': 0.03, 'n_estimators': 230, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 314790.68                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 660, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 302666.29                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2300, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299691.78                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2400, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299414.87                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 299910.95                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 300043.06                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299784.07                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2700, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299325.64                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2200, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 328590.36                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 1890, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 1.0}\n",
      "loss 299882.58                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 1570, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299524.60                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299288.42                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2100, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 300447.92                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2570, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 299816.72                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299081.79                                                                  \n",
      "{'learning_rate': 0.05, 'n_estimators': 2240, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301402.03                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 1.0}\n",
      "loss 300212.05                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2610, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 1.0}\n",
      "loss 299716.59                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1950, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300094.14                                                                  \n",
      "{'learning_rate': 0.26, 'n_estimators': 2260, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 332986.30                                                                  \n",
      "{'learning_rate': 0.22, 'n_estimators': 2660, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300363.95                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 2830, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 299587.19                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2340, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299921.00                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2010, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 299347.18                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2450, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 299027.39                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 1480, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 300297.32                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2130, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 300154.18                                                                  \n",
      "{'learning_rate': 0.2, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 1.0}\n",
      "loss 299740.47                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 2760, 'objective': 'regression_l1', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 324836.63                                                                  \n",
      "{'learning_rate': 0.02, 'n_estimators': 1750, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 0.7000000000000001}\n",
      "loss 306501.08                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 1850, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.6000000000000001}\n",
      "loss 300105.41                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2080, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299648.65                                                                  \n",
      "{'learning_rate': 0.07, 'n_estimators': 2380, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 300209.38                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2540, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 299707.11                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300029.94                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1700, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 1.0}\n",
      "loss 300075.66                                                                  \n",
      "{'learning_rate': 0.14, 'n_estimators': 2440, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.7000000000000001}\n",
      "loss 299104.55                                                                  \n",
      "{'learning_rate': 0.06, 'n_estimators': 930, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 0.7000000000000001}\n",
      "loss 303938.95                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2690, 'objective': 'regression_l1', 'reg_alpha': 0.06, 'subsample': 1.0}\n",
      "loss 332620.86                                                                  \n",
      "{'learning_rate': 0.19, 'n_estimators': 2210, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.9}\n",
      "loss 300087.92                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2030, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 299113.72                                                                  \n",
      "{'learning_rate': 0.15, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 299844.39                                                                  \n",
      "{'learning_rate': 0.1, 'n_estimators': 2500, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 299733.40                                                                  \n",
      "{'learning_rate': 0.01, 'n_estimators': 1090, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 312023.90                                                                  \n",
      "{'learning_rate': 0.17, 'n_estimators': 2590, 'objective': 'regression', 'reg_alpha': 0.07, 'subsample': 0.7000000000000001}\n",
      "loss 299687.18                                                                  \n",
      "{'learning_rate': 0.23, 'n_estimators': 2390, 'objective': 'regression', 'reg_alpha': 0.03, 'subsample': 0.7000000000000001}\n",
      "loss 301372.42                                                                  \n",
      "{'learning_rate': 0.09, 'n_estimators': 2090, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.6000000000000001}\n",
      "loss 300094.98                                                                  \n",
      "{'learning_rate': 0.3, 'n_estimators': 2180, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301333.03                                                                  \n",
      "{'learning_rate': 0.12, 'n_estimators': 1980, 'objective': 'regression_l1', 'reg_alpha': 0.05, 'subsample': 0.9}\n",
      "loss 328697.66                                                                  \n",
      "{'learning_rate': 0.16, 'n_estimators': 1920, 'objective': 'regression', 'reg_alpha': 0.02, 'subsample': 1.0}\n",
      "loss 300457.60                                                                  \n",
      "{'learning_rate': 0.21, 'n_estimators': 2320, 'objective': 'regression', 'reg_alpha': 0.04, 'subsample': 0.7000000000000001}\n",
      "loss 301253.97                                                                  \n",
      "{'learning_rate': 0.18, 'n_estimators': 2490, 'objective': 'regression', 'reg_alpha': 0, 'subsample': 0.9}\n",
      "loss 299731.06                                                                  \n",
      "{'learning_rate': 0.11, 'n_estimators': 2860, 'objective': 'regression', 'reg_alpha': 0.06, 'subsample': 1.0}\n",
      "loss 299872.03                                                                  \n",
      "{'learning_rate': 0.13, 'n_estimators': 2280, 'objective': 'regression', 'reg_alpha': 0.01, 'subsample': 0.6000000000000001}\n",
      "loss 299823.17                                                                  \n",
      "{'learning_rate': 0.08, 'n_estimators': 2140, 'objective': 'regression', 'reg_alpha': 0.05, 'subsample': 0.7000000000000001}\n",
      "loss 301054.09                                                                  \n",
      "100%|████| 1000/1000 [2:45:49<00:00,  9.95s/trial, best loss: 298923.7169845877]\n"
     ]
    }
   ],
   "source": [
    "best = fmin(rf_objective, space, algo=tpe.suggest, max_evals=1000, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18f3514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(doc, pref=''):\n",
    "    res = {}\n",
    "    for k, v in doc.items():\n",
    "        k = f'{pref}.{k}' if pref else k\n",
    "        if isinstance(v, dict):\n",
    "            res.update(flatten(v, k))\n",
    "        else:\n",
    "            res[k] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "819a28ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>tr_loss</th>\n",
       "      <th>params.learning_rate</th>\n",
       "      <th>params.n_estimators</th>\n",
       "      <th>params.objective</th>\n",
       "      <th>params.reg_alpha</th>\n",
       "      <th>params.subsample</th>\n",
       "      <th>train_time</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>298923.716985</td>\n",
       "      <td>228704.235626</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2360</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.504366</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>298923.716985</td>\n",
       "      <td>228704.235626</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2360</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.490871</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>298962.620421</td>\n",
       "      <td>229039.443760</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.433737</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>298962.620426</td>\n",
       "      <td>229039.443816</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.884760</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>298962.620426</td>\n",
       "      <td>229039.443816</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.703802</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss        tr_loss  params.learning_rate  params.n_estimators  \\\n",
       "634  298923.716985  228704.235626                  0.13                 2360   \n",
       "579  298923.716985  228704.235626                  0.13                 2360   \n",
       "914  298962.620421  229039.443760                  0.13                 2310   \n",
       "746  298962.620426  229039.443816                  0.13                 2310   \n",
       "907  298962.620426  229039.443816                  0.13                 2310   \n",
       "\n",
       "    params.objective  params.reg_alpha  params.subsample  train_time status  \n",
       "634       regression              0.04               0.7    4.504366     ok  \n",
       "579       regression              0.04               0.8    4.490871     ok  \n",
       "914       regression              0.04               0.7    4.433737     ok  \n",
       "746       regression              0.05               0.9    4.884760     ok  \n",
       "907       regression              0.05               0.7    4.703802     ok  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(map(flatten, [e['result'] for e in trials.trials])))\n",
    "df.sort_values('loss').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4273efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "1/0\n",
    "with open('trials.pkl', 'wb') as f:\n",
    "    pkl.dump(trials, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7eab8880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABKhklEQVR4nO2dd7xU1bX4v0uqoII1gvgExUaIQFRiXizYImrUl2Ih5meiRn155qW+RH0mUROT5zM9amyJMe1hbyiKDQUrRQGpCggCgpfeb1+/P+bMzJmZc87sc+bMnXsv6/v5XJizz26n7bXXWruIqmIYhmEYSdip1hUwDMMwOi4mRAzDMIzEmBAxDMMwEmNCxDAMw0iMCRHDMAwjMSZEDMMwjMSYEDEMwzASY0LEMAzDSEyHFyIiMkpEJovIHSIyqtb1MQzD2JHoWi6CiPQEJgE9vPgPqep1AfG+C3wdUOAd4GJVrY9bIRG5B/gcUKeqQ4vOjQZ+D3QB/qSqN3nlbQF6Asuj8t5rr7104MCBcauUY+vWrfTu3Ttx+o7IjnbNO9r1gl3zjkIl1zx9+vQ1qrp34ElVjfwDBNjF+90NeBM4pijOfsD7wM7e8QPA14ri7APsWhQ2OKC844FPArOLwrsAi4ADge7ATGAIsJN3/mPAP6Ou5cgjj9RKmDhxYkXpOyI72jXvaNerate8o1DJNQPTNKRdLWvO8vLY4h128/6CFtzqCuwsIl2BXsCHRedPAB4TkR4AInIZcEtAeZOAdQH5jwQWqupiVW0E7gPOUdVW7/x6MtpSCSJylojctXHjxogrNQzDMOLi5BMRkS4iMgOoA55T1Tf951V1BfAr4ANgJbBRVZ8tivMgMAG4X0QuBC4Bzo1R1/2AZb7j5cB+IvIFEbkT+Dtwa1BCVR2nqpf36dMnRnGGYRhGOZyEiKq2qOpwYAAwUkSKfRW7A+cAg4D+QG8R+UpAPjcD9cDtwNk+DScxqvqIql6hquer6kuV5mcYhmG4E2t0lqpuACYCo4tOnQK8r6qrVbUJeAT41+L0InIcMBR4FChxzpdhBbC/73iAF2YYhmHUiLJCRET2FpG+3u+dgVOB+UXRPgCOEZFeIiLAycC8onxGAHeR0VguBvYUkRtj1HUqcLCIDBKR7sAFwBMx0huGYRgp46KJ9AMmisgsMg35c6r6JICIjBeR/p6P5CHgLTLDe3ciIzD89ALOU9VFnjP8ImBpcWEiMhZ4HThURJaLyKUAqtoMfJOMX2Ue8ICqzol9xYZhGEZqlJ0noqqzgBEh587w/b6OCBOVqr5adNwE3B0Qb0xEHuOB8eXqbBiGYbQNHX7G+o5Kc0srD0xdRkurbW9sGEbtMCHSQbn3tSX88OFZ3Df1g1pXxTCMHRgTIh2UtVsbAdiwranGNTEMY0fGhIhhGIaRGBMihmEYRmJMiBiGYRiJMSFiGIZhJMaEiGEYhpEYEyKGYRhGYkyIGIZhGIkxIWIYhmEkxoSIYRiGkRgTIoZhGEZiOoUQEZFRIjJZRO4QkVG1ro9hGMaOguse6z1FZIqIzBSROSJyQ9H5Q0Vkhu9vk4h8J2mlROQeEakTkdlF4aNFZIGILBSRq32nFNgC9CSz97phGIbRBrhqIg3ASao6DBgOjBaRY7InVXWBqg739mE/EthGZgvcAkRkHxHZtShscEB591K0Ba+IdAFuA04HhgBjRGSId3qyqp4OXAUUCDjDMAyjejgJEc2wxTvs5v2FbWRxMrBIVUt2LQROAB4TkR4AInIZcEtAeZOAdUXBI4GFqrpYVRuB+8hstYu3UyLAeqBHcX4icpaI3LVx48aIqzQMwzDi4uwTEZEuIjIDqCOzRe6bIVEvAMYGnVDVB8lsb3u/iFwIXAKc61iF/YBlvuPlXhgi8gURuRP4O3BrQLnjVPXyPn36OBZlGIZhuFB2e9wsqtoCDBeRvsCjIjJUVYt9Ft2Bs4FrIvK5WUTuA24HDvJpOIlR1UeARyrNxzAMw4hH7NFZqroBmEiRz8LjdOAtVf0oLL2IHAcMJeMzCd2TPYAVwP6+4wFemGEYhlEjXEdn7e1pIIjIzsCpwPyAqGMIMWV5aUcAd5HxZVwM7CkiNzrWdSpwsIgM8jSeC4AnHNMahmEYVcBVE+kHTBSRWWQa8+dU9UkAERkvIv1FpDcZ4RJlVuoFnKeqizxn+EVAiQNeRMYCrwOHishyEblUVZuBb5LxqcwDHlDVOY71NwzDMKqAk09EVWcBI0LOneE73LNMPq8WHTcBdwfEGxOSfjwwvlx9DcMwjLahU8xYNwzDMGqDCRHDMAwjMSZEDMMwjMSYEDEMwzASY0LEMAzDSIwJEcMwDCMxJkQMwzCMxJgQMQzDMBJjQsQwDMNIjAkRwzAMIzEmRAzDMIzEmBAxDMMwEmNCxDAMw0hMpxAiIjJKRCaLyB0iMqrW9TEMw9hRKCtERKSniEwRkZkiMkdEbgiJ11dEHhKR+SIyT0Q+nbRSInKPiNSJSPH2u6NFZIGILBSRq32nFNgC9CSz93rVWb25gc31TW1RlGEYRrvFRRNpAE5S1WHAcGC0iBwTEO/3wDOqehgwjMzGUTlEZB8R2bUobHBImfdStP2uiHQBbiOzBe8QYIyIDPFOT1bV04GrgEAhlzZH//x5TvzVy21RlGEYRrulrBDRDFu8w27en/rjiEgf4Hjgz16aRm8vdj8nAI+JSA8vzWXALSFlTgLWFQWPBBaq6mJVbQTuI7PNLt4uiQDrgR7lrikt1mxpaKuiDMMw2iWue6x3EZEZQB2ZrXHfLIoyCFgN/EVE3haRP3nb5eZQ1QfJbG17v4hcCFwCnBujrvsBy3zHy70wROQLInIn8Hfg1oD6nyUid23cuDFGcYZhGEY5nISIqrao6nBgADBSRIYWRekKfBK4XVVHAFuBq4vioKo3A/XA7cDZPg2nIlT1EVW9QlXPV9WXAs6PU9XL+/Tpk0ZxhmEYhkes0VmeiWoiRf4KMlrBcp+G8hAZoVKAiBwHDAUeBa6LWdcVwP6+4wFemGEYhlEjXEZn7S0ifb3fOwOnAvP9cVR1FbBMRA71gk4G5hblMwK4i4wf42JgTxG5MUZdpwIHi8ggEekOXAA8ESO9YRiGkTIumkg/YKKIzCLTkD+nqk8CiMh4EenvxftP4J9evOHAL4ry6QWcp6qLPEf4RcDSoAJFZCzwOnCoiCwXkUtVtRn4Jhm/yjzgAVWdE+NaDcMwjJTpWi6Cqs4CRoScO8P3ewZwVEQ+rxYdNwF3h8QdExI+Hhhfrs6GYRhG29ApZqwbhmEYtcGEiGEYhpEYEyKGYRhGYkyIGIZhGIkxIWIYhmEkxoSIYRiGkRgTIoZhGEZiTIgYhmEYiTEhYhiGYSTGhIhhGIaRGBMihmEYRmJMiBiGYRiJMSFiGIZhJMaESA3462tL+N4DM2pdDcMwjIrpFEJEREaJyGQRuUNERtW6PuW47ok5PPKWbcpoGEbHx2Vnw54iMkVEZorIHBG5ISTeEhF5R0RmiMi0SiolIveISJ2IzC4KHy0iC0RkoYj493BXYAvQk8xWvYZhGEYb4KKJNAAnqeowMjsWjhaRY0Linqiqw1W1ZHMqEdlHRHYtChscks+9FO3jLiJdgNuA04EhwBgRGeKdnqyqpwNXAYFCzjAMw0ifskJEM2zxDrt5f5qgrBOAx0SkB4CIXAbcElLmJGBdUfBIYKGqLlbVRuA+Mvu14223C7Ae6JGgboZhGEYCnHwiItJFRGYAdWT2WH8zIJoCz4rIdBG5vOSk6oNk9ke/X0QuBC4Bzo1R1/2AZb7j5V4YIvIFEbkT+Dtwa0D9zxKRuzZu3BijOMMwjPiMm/khv5qwoNbVaDOchIiqtqjqcGAAMFJEhgZEO1ZVP0nG3HSliBwfkM/NQD1wO3C2T8OpCFV9RFWvUNXzVfWlgPPjVPXyPn36pFGcYRhGKP859m1unbiw1tVoM2KNzlLVDcBEivwV3rkV3v91wKNkzE8FiMhxwFDv/HUx67oC2N93PMALMwzDMGqEy+isvUWkr/d7Z+BUYH5RnN5Zp7mI9AY+CxSPrBoB3EXGj3ExsKeI3BijrlOBg0VkkIh0By4AnoiR3jAMw0gZF02kHzBRRGaRacifU9UnAURkvIj0Bz4GvCIiM4EpwFOq+kxRPr2A81R1kecIvwhYGlSgiIwFXgcOFZHlInKpqjYD3yTjV5kHPKCqc+JesGEYhpEeXctFUNVZwIiQc2f4DoeVyefVouMm4O6QuGNCwscD46PKMQzDMNqOTjFj3TAMw6gNJkQMwzCMxJgQMQzDMBJjQsQwDMNIjAkRwzAMIzEmRAzDMIzEmBAxDMMwElN2noixY7JxexNdd5JaV8MwjHaOCREjkGE3PEuv7l3440k9a10VwzDaMWbO6qC0hY6wrbGlDUoxDKMjY0Kkg5JkVzDDMIy0MSFiGIZhJMaEiGEYhpEYEyKGYRhGYkyIdFDa0+DbBas2s2LD9lpXwzCMGtAphIiIjBKRySJyh4iMqnV92oL25Fg/7XeT+MxNL9a6GoZh1ACX7XF7isgUEZkpInNE5IaIuF1E5G0RebKSSonIPSJSJyLFW+yOFpEFIrJQRK72nVJgC9ATWF5J2YZhGIY7LppIA3CSqg4DhgOjReSYkLjfJrN1bQkisk92H3Zf2OCQfO4FRhfF7QLcBpwODAHGiMgQ7/RkVT0duAoIFXKGYRhGupQVIpphi3fYzfsrsaaIyADgTOBPIVmdADwmIj28+JcBt4SUOQlYVxQ8ElioqotVtRG4DzjHi9/qxVkP9Ch3TYZhGEY6OC174mkB04HBwG2q+mZAtN8BPwR2DTiHqj4oIoOA+0XkQeAS4NQYdd0PWOY7Xg58yqvfF4DTgL7ArQH1Pws4a/DgMMWn49GeHOuGYey4ODnWVbVFVYcDA4CRIjLUf15EPgfUqer0MvncDNQDtwNn+zScilDVR1T1ClU9X1VfCjg/TlUv79OnTxrFtQvak2PdMIwdl1ijs1R1AzCRIn8F8BngbBFZQsbMdJKI/KM4vYgcBwwFHgWui1nXFcD+vuMBXphhGIZRI1xGZ+0tIn293zuTMUHN98dR1WtUdYCqDgQuAF5U1a8U5TMCuIuMH+NiYE8RuTFGXacCB4vIIBHp7pXzRIz0HYbJ763m6J8/z3ZbANEwjHaOiybSD5goIrPINOTPqeqTACIyXkT6O5bVCzhPVRd5jvCLgKVBEUVkLPA6cKiILBeRS1W1GfgmMIHMCLAHVHWOY9kdiv8ZP5/VmxtYtDoVa59hGEbVKOtYV9VZwIiQc2cEhL0EvBQQ/mrRcRNwd0i+Y0LCxwPjy9W5Wize0MLXrn6KyT88sVZVMAzDaFd0ihnrbcUrK5oBeGlBXY1rYhiG0T4wIRKDjY2ZMVFNLTY2yjAMA0yIxGL6RxlHd3Nra5mYhmEYOwYmRBIQVxOpb2rhT5MX09LauTSYFRu28+5Hm2tdDcMwaojTjHWjkOaYQuS2iQu55cWF7NazG+cdvX/5BCE8N/cj5n64iW+fcnBAnVrp2qVt+wS2cq9hGKaJJKCpJZ45a9P2JgC2NTZXVO5lf5vGb59/tyT8neUbGXzt0+bwNwyjzTEhkgC/ELl/6gc1qYN/7aypSzJrVb60YHVN6mK0T7Y2NDNr+YZaV8Po5JgQSYDfJ3LVw+/UpA6dy7tiVINv/PMtzr71VbY2VKYBV4OpS9bx9gfra10NIwXMJ5KAuKOzsg3+9ePmsnvv7ulXyDACmOE10nF9eG3BuXe8DsCSm86scU2MSjFNJAFxfSJ+fv1sqU/DMAyjo2JCJAGNzcl7dmIbgRhtRPvTP4wo7n31faYvLd6LL5yWVmXNloYq1sgNEyIJaA+TDcNkkary5KwPaWi2FYA7It9/YCYvzv+o1tUwasD14+byxdtfd45/09PzOOrG51m/tbEgfFN9E3Wb6tOuXigmRBJQbM7aVN/knDYtRSSsl/nKwjV88//e5pfPLEipJKMtefit5Vxy77TY6VpbtWTVZ1N6OzfPzc10NjZsL2x/TvrVS4z8xQttVg8TIgkoNme9tnBtZHytwK5ww7g5XHCXe+9kw7bMC7VyY9v1RNJmU30TB187nknvVm/I8rfGvs2V/3wrtfyaWlp5YV7tNIi7Jy/m5F+/zOwVG2tWB6N9sGZLY/lIKWJCJAGNFTjWl6zdFnquOSDfv7y6hDcWu9tJK/W5zP1wU00bQ4D5KzfT1KLc8uJ7LF27teJJmkE8MfNDnnpnJdsam1OxK//62Xe59K/TeG3RmhRqF5/pSzMjsZav316T8o0dFxMiCaivwo6Dy9ZtY/C1T/PQ9OU1dYie8YfJXPrXQnNKa6vym+feZd3W6vZwlq7dytDrJrB07dZc2Am/fImv/zW+eceVz/3hFY668fmK8/lgXabO67e6mzarTUd2rNdtro/lZDZqR6cQIiIySkQmi8gdIjKq2uXVV8Fp/V5dZiHDp2Z9GHj++bmF2kFb2rtfWbiGP7zwHj96rLoTKx9+awVbGpp59O0VBeGvLYo2F1bC4jVby0eqMo3NrSxYVbiQ5aR3V3PNI7PSKaADOkfOuuWVWE5mo3a47LHeU0SmiMhMEZkjIjckiRMHEblHROpEZHZR+GgRWSAiC0Xkat8pBbYAPYHllZTtwprN1R1Wl/3m/aaRr/+tsDdeSS/ztYVrYi3Xkh2NVq093zdua+Kfby4tcR6Jr/Wrb2phteN9b2ppjT1Lu25zPc/MXukU995X3+eI6yfEyj+Knz05l9N+N4nl6/OmzovumcLYKcvSKaADqiQfbar90FXDDRdNpAE4SVWHAcOB0SJyTNw4IrKPiOxaFDY4pMx7gdFFcbsAtwGnA0OAMSIyxDs9WVVPB64CKhJgLnwY02mtCb/iX4yfnyhdWJnvr9nK7S8t4st/etN5uZbnljQxf5Xbcu8vzv+Ius3xHfo/eGgm1z46m3cinMJf+8sUjv55sNlpa0Mz9U15AffVe6bw8euCG/kHpy3LrTXm5yt/epN//8dbNDrM7r5+3Fw21afnp8n6M7KDIpIQVGvJneuAUsToMLjssZ7t5QN08/40bhzgBODfReQMVW0QkcuAL5ARCsVlThKRgUXBI4GFqroYQETuA84B5qpq1iO9HuhRnJ+InAWcNXhwmMyqDtsbWyIbxiCUyjqOEmG7+PLdb4SO2lq1sT5wNNQ/5zfCfLfhwpfcO43B++zC8987wSn+P95Yyh8nLqRf352B/ICF1qxG4ruUqMEFH79uAgP37MVLPzgRiDZ//eChYBPRsnUZh3QlI+mK2bCtkRnLNjDq0H1SyW/CnFUsW7eNrx93YGicoIEVaV6TYRTj5BMRkS4iMgOoA55T1TfjxlHVB4EJwP0iciFwCXBujLruB/j1++VeGCLyBRG5E/g7cGtxQlUdp6qX9+nTJ0ZxcQj+Sn/w0EzOu/N1VjloLlGNf1psizBHXXTPm/zwYTcb/B9fWsjAq58KPLckho/hR4/NjtbqYjR+UaPe2oKghvqyv03ja3+Zysbt6Tjbr/j7dG58al7setRahlR6/bNXbGRhXaaP2tzSippUbFc4CRFVbVHV4cAAYKSIDE0Y52agHrgdOFtVtxTHSYKqPqKqV6jq+ar6Uhp5psHclZsA2NoQz5fgIk7SFjlxxpb/akLbTGQMMsOoaugOkQ3NLQVmrVqzaHVGoPqHbj/y1nK+fd/bznm0tio/fmw275cRzkHvQ5pN7Xsfbebga8ezbF08Yf3yu6sZdsOzvLYw+dDnz93yCqf85mW2NDQz+NqnufXFhYnzSoON25pKBkJEsb2xhaffcfO3pU1ra/j3khaxRmep6gZgIkX+Ctc4InIcMBR4FLguTtnACsC/LeAAL6z9UsMOU0furEXV/c+vvM9B/z2eDdtKhd5nbnqRw378TBVrVoi/Rxw1P6dFlRE/fZYHpy3jew/M5PEZwSPwgq57/qrN/P2NpXzjH9Od6/WPN5Yyzef3qaTn/uGG7TQ0t3D/1GU0tSjPzF4VK/2U9zOmxbdSWPY9u7zH/dMKBxw88tZyvjW2VDBvqm9KbRn8+qaW3Dv3+T++ymm/mwTAotVbGHj1U7y2aA3vfbSZm58p9WPeMG4O3/jnW6ksff/LCfO5baK7EL3879M56L/HV1xuFC6js/YWkb7e752BU4H5CeKMAO4i48e4GNhTRG6MUdepwMEiMkhEugMXAE/ESF8Rv3kuavXdaL1g+Ybg3ltaanm2Abv3tSVc+X/pzcJuj9w/NdOA1G1uYODVT/HrZ/NaUbE21dzSykeb6pnz4UaOuH6C8+guV8Ie38CrnyqY29LQ1Mr6bU385PE5qZYfxo8em82X7sgPj437ljW3tLJuayPNLa38600v8p37ZsSug6vZqbVV+dpfpvBqBZrK9x6YyRMzSwXzEdc/y4ifPZc4Xz9fvP01hv80k5d/WPgbizNCctzMlXz5T2/yx5cWlaTNTgDd4gk0l8mzm+qbArXq2yYu4pdFloCo+/x8G0wcdtFE+gETRWQWmYb8OVV9EkBExotI/6g4PnoB56nqIs8RfhGwNKhAERkLvA4cKiLLReRSVW0GvknGrzIPeEBV2+arBP7wwnsRZ4MfYjY067QtOR/y7F0++mycCXNWMb5GqnIUra3Kt8a+XVHvy0XG3hJh2rjuiTl86hcv8Lvn32NTfTMvOyyjEqfBjYqb5OMNNN9VoM7mRmfFzOJnT87lkz97LjcC7YV58bZdVlUGX/s0N4ybWzbu5vpmXlqwOpamFYfG5nQWS53z4aaycVzMRo/PWMGQn0xg3sro/I64/lnOuuUV5/rVEpfRWbOAESHnzvB+fhgWxxf31aLjJuDukLhjQsLHA9XVzVIkjqaRbSzqm1pye7K7MGt58OivWpuz1mxp4ImZH/LaorVM+9EpkXGL71NaVc825HGWTYklRFR5bm4d3brE91CpKv94Yyn/NmI/X1hQvMz/UsF6NnEF0XjPZOU3BcW7L5n/731tCVeeeFAufGtDMys31jN4n11i1SeIDdsauW3iQn44+rCK86oWt01cyH+MOqggLLuF9byVmzi8326R6d+rc3MZV/JupEGnmLHeUQn6MN9YvI4VG8qvf5TktSl+1wZe/VSgGSAVcmWVb37CYrR3t46SH4EVl9cXr+XHj8/hOp+JK+3rrcb9c2mvwsr96j1TOOU3L6dSjxufmsfdk9/n6Zg+mrbklxMWOAuCjowJkVQI/rK213CkkGvv86+vLalK+Tt5rY2LRhRq1mtDdSqJ2aiS6mXt3eu3NeYa5taADLNh/jesvqmFG8bNYXN9Exfc9TqX/21adO1Tvo13T1rMiJ8+C2RMONmtEKa8v47N9U2hz23a0vT2VM9ux9Ba5ZFHlVLtkVHtARMiqVD6ojQ2t5ZduqGjjXdvaG7loeluq8pkG72ghrEYzTWUXqoOMqotjZng/hyizFl+Hpi2jL+8uoTfPf8ebyxex7O+ddUCJxsmrVtIwp+Pn8d6b3b9tY++wxHXP8u6rY2cd+frXPl/b8f0K7Xfb2DSu6v5vzfdlweKyzvLNzLw6qdYWOc+XLg9YkKkSlRjkcYsC+s2szSlyXXbG1v4wwvvOe0b/9qitfzXgzNx6Vxl7bRxBgm4hkP1fD5JbP9pxQsqPaeJ+IRDs7c0S7leblLHer425RM+/FamU5H1n8xbuSlxea8uXMPAq59i/qryTuwwZq/INMzZUVOVcNE9U/jvR6u36Og4b7HVcgMXhvzkGc67o/0uRlnWsW4kw8VnkbQdPOU3kxKmLGXuyk25SZFpEqcBy8YpbrSi0rbnHmwxQXUNWqGgWrPNk96rOMLAL+T85bnmISK5OShT3k8+xyU7VPjF+fFGlLVntjW2MCVgvbf2gmkiVcJlxIT/+2iLZU/acgxH9tJczFm5NHFMSdXSRKpQB+d4EWmTDMCp9BY5aZFa+H/x7yz+7yHWqMWO01eoGbU2i5sQqRK13sKh1h+f5tWL8nHLzLMJCqv19YF7D99VkAZHS8HvktSc5eLPKirDSQOP5XcyXPiPf05n9O/Ss1DEwcxZVcLNF9B5PxEt+j8ybqhTJGDynReW9r3LaoLVeCKuA3SCGu3WXOPs1i1J0ySWJJ2G1CEs37Jx20NvIYLy9Y86l961jX+ndkOdTROpEnHMOJ2R7OW7jc4qOo5ovqqtiVTHsR4gDB1LcvYpxCw/rbKDhHq5a/PXp1wRHeUrquV8v1rfIxMiVaKS+REdmeZW5bAfP807KzYAjveh+DjKElbte1YFU0uUJuJve4KiBY3OcqXS0VkuVxgk1ONpIuUEjltYUN7VxkU4uzw3kcq0klq3IyZEUiDwIdbaJxEQ1lbLI9Q3tfLnV9736uGiiQQ3lFGmmfahibj1uCvxiVTklHdLGp5nDFNTgWCIkW9H7kel+Q5Wlpc51js8wR9wR/48KidvzkqQNjLf6t7XOM/NNWbYEGaXsvMTMd1Is58QRxj4BWVZ4RpjCHDc76gWIxBrnZdpIp2AoJ7mjmrOKqGC+xDUgLQ6Nsgu+YcUmnq+znVN0SlekEfCTMppUGEO8niaiLs5q719L3E7NOHL+1RozkqcMh1MiKRAtSaJpU2brkUVo7GP1/vXgvxdiDVXxT3bgshRcyMqGcGT328+38eOq20k1YpbyyxioKq+urtrF4WZxInavr4ql9qohj+vgrkzldTDNJGOT7Amkm7j2dHI3hMXc1bxMNaoexdj+okvv/JxEy3AmPY8kaCwFF6RttZEyt2WxFpLO/tcKq1bgU8pQfr80kLmE+nwtEdNpNYfXN4B7iBMQ+JEOZrjCOlq3Qp/FaK0A/d5IkFp4/lE/ORGZyVIG1afsPMtBU726vhEav1NFaNoYh9UOp2D9nFHOoUQEZFRIjJZRO4QkVFtXX7UEt5RFERpA49gLTavcVL5c/8Xxq50KKfmM3amGpYYV00kUKONUZ8004KLJpI/7zd9xRE+8Xwi7aPRzJJIewgKk8q0iVrfFichIiI9RWSKiMwUkTkickNAnP1FZKKIzPXifDtppUTkHhGpE5HZAedGi8gCEVkoIld7wQpsAXoCbmuVp0hgT9PJhFJN2ocq4vSChzkc06lC1Zzwro1apHmOvBYTGKuGPdayQsR32q8xlSutcCRXdNxYQr3WrWkFVFL1Wl+2qybSAJykqsOA4cBoETmmKE4z8H1VHQIcA1wpIkP8EURkHxHZtShscEB59wKjiwNFpAtwG3A6MAQY45UxWVVPB64CSgRctQmekbxjU4kzOyplboa0i5COETcJ7pqIY34RGm0lSmTSyy9X7zCfQPkhvsG/g8twN2eptu13V+vGO0uH8Ilohuw+j928Py2Ks1JV3/J+bwbmAftRyAnAYyLSA0BELgNuCShvEhC09vFIYKGqLlbVRuA+4BxVzSrT64EeLteUJkkniXXknlOaxNuL3j1NHCd8cf5OcR0jB+2+5/rOOA8jjjL9JXzNyjl+C8xZMRv7oDIC44akKxe3LQy3aY487MiaiPMCjJ4WMB0YDNymqm9GxB0IjAAK4qjqgyIyCLhfRB4ELgFOjVHf/YBlvuPlwKdE5AvAaUBf4NaA+pwFnDV4cJDSUznBtuzaPtlav1iVNNxRzvNskEvvPo7AKc4/TYLqWq5RzqfNm4mSk9ScVSbXAHNWcXi56sQb/lzOvOYuyNIgzXel2u1FNTuszo51VW1R1eHAAGCkiAwNiiciuwAPA99R1ZLdjlT1ZqAeuB0426fhJEZVH1HVK1T1fFV9KeD8OFW9vE+fPpUWFYhrI1ESJ/2qRNKms3kTvLQumkP2Y4uz7W5+CHH5suPg3st0M3cGzctwNQtWY8xEnCG+rb5nl2RmvkshafpP0qDS8tIavuxm9Uiefzlij85S1Q3ARIJ9Ft3ICJB/quojQelF5DhgKPAocF3M4lcA+/uOB3hhNSVwuYoa1KM9kWS5kywuAshJiBT9n1bckkREf6TBnYxwrSMobdDIuihNrVyYC+Ud6xr4O+gmhpnGYg1kKHde27ajlGSF6tB4CcqPM0+kmu2R6+isvUWkr/d7ZzImqPlFcQT4MzBPVX8Tks8I4C7gHOBiYE8RuTFGfacCB4vIIBHpDlwAPBEjfVVwbSRK41ShMh2Q4vuQN2eFx43V+6qSWhiUJEgjcB26G/QeldtH3aVWsXMIuMdB1+XP11/PGNasxHNKXM2BaVPpBMEgku5qmmSQSTVw1UT6ARNFZBaZhvw5VX0SQETGi0h/4DPA/wNOEpEZ3t8ZRfn0As5T1UWeM/wiYGlxYSIyFngdOFRElovIpQCq2gx8E5hAxnH/gKrOiXnNqePaIyyNlH5d2iDr1MsvXsrEZT+ROOYst0fh/kHm83cLc11bLXJ0VkD5wdqJW51ciDfE1728OA1xoYJTGrlgb/c2eOkLykhgRgpLUv2FRauHk2NdVWeRcZQHncsKig8po02q6qtFx03A3QHxxkTkMR4YX6bKbYrr6Jsdigqc2VnfQPCqtl6cGMpFmiaTwrj52NnGzH0xznDTVWFYvBcpKHbitbNiqBSFo7PKaRfBv8vGbQffVBwtKipOgfCrUIdy6iRV8d51ihnrtSbph1vrEVzVJFZjXKSBRDf+mUCnxjUk37QINqmU4ipYAn0inkB1dZyn2VgEdY4Kygob4htHu4jld3HPt1rENWdFbkiWkhZV63X6TIikQNLRWW1NW656Es8spJHHQfm6fTjuaZKNzgrKx03DcBU2LTErFqW9xSWJOStsxnrhirV+rSWaOD3/Snv0LsS0ZrmbqSoRIg5lmybSzkk6Y709Cpq0SDJjPW+qimjss/mXWaa8IE1OIwknW2Ylwq9c3kF1Kh+WCSznfI2qSnIhEn3efzrOplSxhu2WiRt2vlr9pThaFES/c+X8PUnq5BKeNiZEUiD5UvBhB5VT69nwSRzUxUmievqJHOtRDW35KAH5u4YFdTKCNJYATSSGsAwtP7FPxKdeBJYVLDhiyJCSo9K4wefLjRarFnFXFXZWRCqqfHDiAq3JNJH2jattPDKPTuYfidUY50ZGldcGcpqIQwF5DScrTNI1ZwWX6WjOCgiLdKy7+kSCwhJeW5wlSZJohsW/yxUSa9SXe3ViEavujjURpDLHeqgm4r8f1WtfTIikQOIZ621ks6wFSZYaKTZnRQ2dTjIPJ4Yv3glXTcR1nojrBMToOqX3IlVr2ZPq+UTaFqfBMwUmK8d8HZ9hfrJhSD4h9UgbEyIpEPyhx/3406lLNG2/n4gLxaakKNNS8VIm0fkWajYpdRxL8veXEUTUZFRV31LwQQLISxz05BLPT3Ik1n4iGhwemC6WdlH6O2oTs2q/4QVFx+yURA/siN+hLD+yLbgeaWNCJAVcR+SUpAv5vaNR0jg4mbPce4FxJhJW4zlU4oAPeo+iGkr3eSou9SkToaCBj6OJ+H+7C6qyl9EGH1Fcn0ir40x+l7bg5XdXB6cNM2cVdHLMnNWucTVrROeR7kOutVCKdznq+9fXEEYoeLHWzioSJi5pXCg/WiirNbkKkaAwTxMpmJxWSn6yY0CdEr4NsXwiMXq9Sc245etT/bc+rk9EQw+i8g2OOHF+XUjadN/ruJgQSYGkH25bqZu1INYQXy36n8L/o9JEZ1yUX8o32Z9d3Bnrrr6TuJpFupqIu7kk58cippkljjkrOmqbmITjaFFQXP/C+GF5xb2MsPjxBwEkw4RICiT9cF1t6kmotaM+Vo+++Dgtc1bxqC+XusQSfgHPPSCe63ImUWZR/zyRaOHqVicXyo24SropVVgewed9vx3MZNV+7eNqUc7feAUNfpwOVTUwIRKTUw7/GN27FN62ajs4OzutRY189OisrImofL4lVrGU1f5Aa5ujNhEsWILC3IRNsTYXWsEYxNFEWnwDAJI4y8Pjul+QP27VJhs616Y0Uti1irhpOKEDCkJ8H1FaUJqYEInJ/nvszE1f/ERBWGBjEnfoRhsYtNp22ZME1+MbsQRh9zVDHJ9ItW5tsEkqKJ6rYAkIi7kUvKtgc8qrTDJ/fVtjvMtxGrRYAsc513iE74USr1MSZXZyuc7Q0nwnwkbJmTmrnZOGHbqa9vpaEM9h6v0fJ67T6KxCDSf9e5Jc63Ad0ZfflMqfNrxGBeVnhw6HR4+k3NyPIJ9I5nd0vklt9eWiVquhDFOGXMqLWg6m0j5d0DyRcE2kepgQSUBxjz7pLn4pWh46NMUz1aPWscpvj+uQb0j+0XUpHycqrr9e2Y+8EsEStClVVE8+zcVAA4cXh6izVRviG0sTqc5XFFbfuO9TVHSXfF3C/c9s7JQPfHHMnNVuUC1dDM/VNBGUV9DvzkCcy8lrCplfKzfWFxwXxC0SNC7k9v9O+SYX9gDDy3D1dUT5P1w1keBrTHbdiTeliuG7iCMYYi3sGB0zFpX4GYLekeAy4uVbXK+gtDc+NS/3u5LtqsthQiQBxZ2xcqq+C51u7awEdgqX9Zeyucb5KOLsJxJL+Dk+d+fl4QPCopaCDzrTGmfCRhnizMuIt+xJ8O/gOsSIS3Uc6mH1ddNEwq+2knwL83HQYsyxHo2IjBKRySJyh4iMauvyEy8FX0XHV1Cd2nLRk2SaiHvkOGtnBY5cKlMXt6q4aRPBO1+6aiyZ/12H+FbbnBWWb7ztcf2/ywmqZPmmSVh943ZKoiwPYfm6LKLoZNGopSYiIj1FZIqIzBSROSJyQ0i8e0SkTkRmV1qpsLxEZLSILBCRhSJyte+UAluAnsDySsuPi6t9O4rOpYdU8YPO+kRiqCJxHPdxHkQ5n0hkWECguwAKr5PrYo8ulF87y1+n8o1dUMrymkh0voWT9Krz0oXNgVHV8u95kSYlpcGZIb5hgspBQPiDw55ZNdsXF02kAThJVYcBw4HRInJMQLx7gdFhmYjIPiKya1HY4JDoJXmJSBfgNuB0YAgwRkSGeKcnq+rpwFVAoJBLC1UtcS4m/XDj9Mg6GvGGcWrB/1mCzFt5n0iCOrloL7Hyy//OL6LoKBwcOx5By57kwsrUKTc6q2qaSD7CTx6fE1yHwHTBvwPjhh4E51XtryjMvBY2fD7MzBe5GGPI7/A65WOFCpFaaiKaYYt32M37K6mSqk4C1kVkdQLwmIj0ABCRy4BbQsoMymsksFBVF6tqI3AfcI4XP9vcrAd6lLumJOzWsysAZw/fr+TjTcMn0tlI4BIpIXhkUoZ4y6qoc5pKH1tF+6kHxIu7KVXSDdKCiGNqipVvwlyCTEOhjXr8aoWXW1BGsFCInTYqXphWElZGyO/CONVrkLq6RPK0gOnAYOA2VX0zbkGq+qCIDALuF5EHgUuAU2NksR+wzHe8HPiUV78vAKcBfYFbA+p/FnDW4MFhik95Bu3VG63fwpEH7M7KjdsLziUdERP0UXQWYgmRgAYBYK9du7NqU31RXA2MG5m/97+T9hIrXzdBECVYypki4gqFcj33OMQZnZWlxDRTJl2lWkuxeakaFDznmDc49BuPSlrm/mXJWUT88UM6HTXVRDIV0BZVHQ4MAEaKyNAkhanqzUA9cDtwtk/DqQhVfURVr1DV81X1pYDz41T18j59+iQuo1XJdW96d+9aeq6kzPJ5trUJqy1nrMchrJcUaM7Knotx77Jx09ZEKtEw/PGyTnNXE1fb+UTKxQgxnZQpMdamVOV8IiHmojQJE4pxv/FCGVKsjwVfp9tgkOh7VFx22sQanaWqG4CJRPg+ohCR44ChwKPAdTGTrwD29x0P8MLaBEVzKvKoQ/cuOOfacJTLvzMRR0Dm5nEU3YOoHnysIb4hmk41cBUsQUN3g+5Zs3ehO4nb6Kw0TatJNBHVyrWLwjqUKS86eSqECY64im2xwAs3YfnTR2idAVp52HdR08mGIrK3iPT1fu9MxgQ1P25BIjICuIuMH+NiYE8RuTFGFlOBg0VkkIh0By4Anohbj6S0tubtrCLCyEF75M859iBL4wT/ToMOZR4LaeTj9rjLZO80ouujbe5OiKDn5zqc11nYePl12cknRKLuS8wZ7lHEcno7hAflW+47Kbs6cBW/oXy+4VpCnGstCI84DhVaFVg3am3O6gdMFJFZZBry51T1SQARGS8i/b3fY4HXgUNFZLmIXFqUTy/gPFVd5DnCLwKWBhUYlJeqNgPfBCYA84AHVHVOUPq0qdtcz9yVm1ixJayBqdyEkPZDjrs3d9rEMgt5sYvTRF1DvCXbs/mVj/undxrd83V87lFDfMWXT1C8ppasJhJdbmBZQeNJndCCOobGStjrjWPOilp7qjQv93zjEK6JlFe7wsxt05euj4gXnCbUfOigiVSTso51VZ0FjAg5d4bv95gy+bxadNwE3B0SNzAvVR0PjC9T5dTJ2qxXbw9+QlFDUV1J+9kHjWxqS5JtSlWYJiqPeJeXbaTTvSfuPpGgMLf8WryXq8Cclb1fAXVK0ydStped0P7++dtey8ctp+2UMR+5jGqqlCifSPl75P+dP3po+nIuPXZQSLzgNHHLKIhTY01kh2fvXTOjhvfoGbxHafCH6/Dwq/TSQ+2FSJLRUyUqfpTZJgUhVSnOWoejsAl6Z5oifCJlR25VeLnl8g/XRKLzbfSNW45aH60kryDBG/a7SqpIlF8jMKmjOcrJP+Kg+YWbz6rXHjgN8TXgjWtOZtqbrweeC2yva62JBLxNxQtHtjuKqpyWJpJvdONXKQr3DaPctINATcQzZxX6RDQ0fq2WPSk6415IYH3zgYU+kdLIS9ZuDUyXJmGjpRSNZ7pzFLpxzXL+OO1ysqGRYd8+Pdmlu783GP1yuz389HqNxdRcE4kTV4PNTZGXEEsTiWfOcm2M/PGi9lh33go34IKbPXNW0Iz1wtiZo4V1paPmkzvWy2gJKZhOAoVewfnSRth//s6XFwemS1URiWj8y5Xjohhm7mNwxCgtI7efiIu2UqaelWBCJAWCHlx8n0i6j7nmQiROI1/0f5ZaaSKu987ddBUUFiRsSuPlHetSEi/o9jwzZ1VJWHJNJKhz5GI6cSebX6GQDP7tLy9ozlPckUyuFAinYq2hnLZWELdI7PueY6jfpeB3uBjKEvbu2n4i7Zzsx7Z6cwO/f/49VDWBTyTdOtVeiMSPG+cDraZPpNlViJRZRDFKA3IVLNnnGNyQlalnxTsbloaVu4XFM9azbKpvCowf3AErvYfgch35BRHT7JSFmdRa1WGpSYf6F+khoe1C+ByQ4LoWl1EtzCeSAtmH+18PzuTld1dz7MF7JZhsmHadaixEEqUpTJWaJhLTnFWJJhJYhBdW35R3KLuauJo8J3RQw1r2cip8BcqN9Aq3v5eG+81OYfkF1yE63+K41XAgRzm6y/pEwtSKojihZsIIARE1OTWyHiljmkhCNODl3tLQnDtOe8XYuAQ1hDu1oV89rDcZnSb6OCz/svl6/7sKHldNxNWv4b4oY2lYkCYSvH1w+MNNvgCj73eZUVQF4bHKiM43jiZy9cOzQrXaSghzdLsU4bYkSaHlwkWgFKbP09wSpe9UBxMiKVBsLhFxfMEinGaVEtQgdenSdlLEX75rz774HkRrInFUkXhpkmgiqqVhQfFyYY7CJjvEt7BHmg1yvK9OsUopt5dJaHMV/9EUheVD42x29dYHG9wLjkOYUNPC/USCvq63fXWKdNCH3NdC/1B5LaM5ZHtQ00TaIUE9w9wudK5SpCC/dJ9yUEPYdafaPO6k7pmodEkc664fUtiHWFqHIHOPm3BwFTbZyYb+fP/2+pLQ+IGkeP9d9jyP9S4HlRHSaD8/76Py2eW0tPS+pzDtQyl/rX97fWmgNllSRoSgCiq7sH75M6GO9chaVoYJkYTsFDTkMsQBF0ZDcytrtjRw+d+msWl7c6r1a0/mrKRDa6MagiT7ibg2LO6aiJuJJ9h0VdqwBNq4W0pNV9kRW873NWETUu76QnONJUOCBLG/DvnfD00vv2lp3rGeHuENvFvHJOgZFuZf/N77foeEh8WvhU/EHOsJ8ffqiy0OzS0O22YC33tgJp8+cE+enfsR7wWM76+EoAagLTWRTfV5oeja2BW//9FrZ7nXJRvV2ScSalcuX4eK5okEhOV6scF2n6oSy2nsC4tTrXJCN+4AkewzdhE4rvgFXWEj7SaeG1pKtUn/ceaelW/8i+9Fbp6ILyzs3a3mjHXTRBLStUvpWkbZh/z2B+tZtm5b2Tzmrdzky6P65qwubamK+HBtvIs/kvXbgoeFBsWNovj5lKMSTSTJ2lnZlQSCqpdttMrXPR2B66f86tTBGa/Z0uBcRqAQKRpGG4ds2m2NLbHSRebp7+n7GmlXTaSxOTvCLiT/onNhjnUXTaYWPhHTRBLSrUte/uaGkHrP73+edl8pP2oF10oI+vhqJUSaHfd4jXMP4i0FH+8eO88T8QsCKQ0LipcPixYsWbJDfJsC7qHrHUguRMqYs0Ly/fZ9M5zLDlSwHMoIza8KjaU/S38HQ3Hr4eeGaReF5zcjC9ZQihO5lBU+2bBs0sSYJpKQrjv5ZxC79hZL+curS4C2cazXSohsrnfz9yTZyMot33j5V+YTyYfll6VwExhB8bJ1aQwSIs4+kWSUW5IkKN/IBjGAOPuJuFCNtrKwpx9fE8l1ACJGV4VrInmKlYyg+UJZraekDDNntT/8mkgaWkTaPYX2JETWb3PboyOJs9wpbsz8XUdnBc8TKY0XvFWAm9kr60RvaKpEE0n2cpWfw1GaZkOxCTKBJuIfWhx71nwVutz+LAs0EceympqDNRH/Pjqhc1H8ZTuUVx/wnhTnkzYmRBLSrUvp0tyVPKi2ECK1WsM3yrfhJ5Y5y30Dwry50TH/ytbOchMOruas7BDfpQE+tmo2DJn6BAiRMv6KYlNguSrm5laFpIm7Km2Vb0lBB0NxEySNLXmfSKCA0MKBOG/5NqzyX3+Dg5axvSk9X5ArJkQS0jXAJ+Lagw1ixYbtFdfJT+EmRsq7H22u+gcWxgafJvLVe6bwv8/MD2yoN253EzZQXce6q0+kpci0kSmjNF4lfpKsAF69udRZ7ayJOMYrrU9AXiG98tCyy5qrostw2dI4LG1apOdYL4zc3JrXUPxntjXmzb/+8HqfgAibqBwmRMyx3g7xayLZj7/JcWhoW7ClsZnG5lZ++/y7dN1JuOXFhTWry9aGzIs9b+UmXn53NS+/u5qHKxyCWdXJhhHP8e9v5Hd09jf6+XkfQSYgN00k7ocetEy6n825ZXji5RuVvz+rzSGLKvopV3bQffjt8+/mfn/jn2+VnG9saeWJmR8G5vfOio1l61SOhXWbGf9OfjXkJ2auyP32C86bnpnPqEP2Lptfo8+x/u5Hm3Ph/vkjT/quZ6tvZNmr763J/fZrIn98aRFL1m7Lpc9SHzIqbdWmerqVrWkyOoUQEZFRwM+AOcB9qvpStcv0z7lYsX47a7c0BI6gqRWq8OL8Om5/aVGtq5LrHZ3++8m5sLqAnnUc4u2xnjVnuaWpb2ohu6TFTjsJH22q583313H2sP78+LHZuXh+P0XeeZ/PJ9vgPPJ2vhEqjg/5hi/u++PeS0/Wm3/krdJ6+yv+7/8obeDjlpy9hPXbmpj03mqn+q3bGu5je/ldtzyi+PLdbxa8n796Ni/UfuR7/jOXbXAzZ3mN/w3j5haEZ4X8G4vXMs1nwvIL5x8+PCv3u8GnZfxywoLcb38NwjSRy/42jSM/1oVRo8pWNzapmbNEpKeITBGRmSIyR0RuqCCve0SkTkRmB5wbLSILRGShiFztBSuwBegJpDfLKAL/PJEla7fx6f95kR5d25d1cKlv17dasr0x3dn4ENwwh5FtqFzb3G2Nzfxi/DwO/O/xtLQqn/rFC3xr7NusL2q8fvx4/vVc653zC6oo89yjXv23NuTvzbNzyy/r4efp2fne8gvz60Ljba5v5r2PNufMIfNXbWLqknUA/Pix2Xztma28tijT41XV3LVkUVWnpTuC+PMr70ee9zfCS9eWn1uVhOKGfuqSdcxaXfhOZu/NsnXbWB0xz6XY7OzSMQl779Z65XxQ5O/aEjKaMdQn4qtDlE+kWiatNFu9BuAkVR0GDAdGi8gx/ggiso+I7FoUNjggr3uB0cWBItIFuA04HRgCjBGRIcBkVT0duApILLzi0L1L4a1rbGkNHV5XK+LMV0mTUw7fp+D4Dy8s5JYX3kuc3569u1daJQCnCaCQafzvnpxp/F5dmDcnPFokuPwf9XSvJ/n4jGAzSxgLVm0uG+fsYf1j5VnMDx6axam/ncQPHsr0akf/bjLn3pHZ6jlrnvvy3W+iqvzt9aUl6ZtaNBcv7YboDxW8F67M+TAzqXf2io0sW7eNc+94nd9MzwuK6UvXcdiPn+Gvry3huJsnxlsNIaeBlgrfcqzZkolfrJWHDYmvDxEQ/jXF7p+6LLS83XpUZ2hNakJEM2TX7ujm/RU/jhOAx0SkB4CIXAbcEpDXJGBdQDEjgYWqulhVG4H7gHNUNfs1rwd6VHwxDnQNWBH3w431JWHdKlg5d/7PRvPwNz5dkt8uPfJWyGED+uR+77VL5tKPd7DTBnHeUQOYed1nA89979RDOP+o/XPHf7zwk6H5fOeUQwqOG1ta+fVz74bEDuaZ7xyX+x334wxjxrINTvGuevid3O+L7pmS+/3TJ+cGRa+ILQ3ltbTPf3K/VMoaN/NDxvls74dc+3TB+cN+/Ay/L9Oo3zoxXd/aotXV15Y/d8srrNiwnc/d8grH3TwxFz5/1Sa+9pcpfPH2jEC97ok5sfPOCqixU8Ib7zCCtjIGeG3R2sDwlQHtC8CEOXkhEmXq2617OxcikNEURGQGUAc8p6pv+s+r6oPABOB+EbkQuAQ4N0YR+wH+p7Uc2E9EviAidwJ/B24NqNdZInLXxo2VO92y/Nvw4A+7V/cu7Noz08jvtUsPfnjaYQB86cgB/PHCT3LliQdF5vvXS0byb8P7849LP0XPbl3Ye5eeBednXvdZ9tol3zP/xRc+kfv98g9G8cx3juOerx7FSYcVagNhHOETQjd/aRh9du7GoR/btSTevx60J//7pSPY1RNgZ3yiHyceWiisLvzUv/DAFZ9m6H6ZPHt17+JUBz/3Xnw0M35yKoftu1vstHE5Z3h/5/tUS3YK2gs2If859u3c7+IJjA3NrZGNUEfmZ+NKOwCjfzeZlxYk86FcccKBgeFjRu5fcLxHgBb9yX/pG5rvmUf0Y9WmYGGRlOw3vv+u1TG3p5qrqrao6nBgADBSRIYGxLkZqAduB872aS+VlPuIql6hqucHOdVVdZyqXt6nT5+A1Mk4+GO7suSmM5l1/WeZ+ZPPMuMnp3LK4fsw9rJjeOf607jv8mN48j+P5bLjD2TJTWfyq3OHccYn+vHZIfuG5vnGNSdzwiF787sLRnDswXsB0K9vXog8cMWn6dW9K1sa8mrtx/v3Yb++OwPQu0dXDtt3N7p22alkxd47vnIkd3zlSO67/BjOGtaffxven4e/8a888c1jS+qRfemOGNCHfXplMsqOPHv5hycy5dqTAfiXPXoVpDtrWH9GDtoDgP/7+qeY+F+jCs5f8plBPH7lZwA45sA9eP2akwri7N6rG6MO3Ye+vTIf3ts/PpUfnHZoQR7HDt6r4HhIv7ywmXndZ/n550teOQB+e/6wguOrRh/G7y8Ywa1fHsF3TjmYm794RGA6P9kG4dJjBwWe/49RB3HUAbvnjg/31e2a0w+jb6/M+JgHrvg0++7WsyQ9wAF75u/pb88fxh/GjGCbp62MKhLaY0buz7D9+wLwrZMPZt5P8xbg7l13itQWXbng6P0jz/fv05MH/z2vLf/63GG88P0TGP+t4yJSZfjRmYfn3pdirjzxoIJ8Ac49cgDXnTUkd3zvxUdzlmfqu+L4fKPu12JzYQF7zyfl/KP257tF2jbA7y8YnrMGZJl67SlA5p35wWmHMv1Hp/DTc4LfUYBbx4zgqtGZjucxBwbfGz9HD8y/b7d+eUTJ+ee+ezx3/r8jefa7x3P0vtUZR1WVXFV1g4hMJOPXKHCOi8hxwFDgUeA64Jsxsl4B+N/qAV5YzditZ37g3J++enTu9zEH7hkYf9j+fVly05n88KGZPDAtPwbg+6cewr59ShuWbl124r2fn86K9dsZuFdvAL518mB+8vic3Af+/PdOYPn6Qnv/pcceyPPz6nj+e8ezpaGF4V5jE1S3x6/8TIET+Gf/NpQLRv4LRx6wO3c+8gJ/fKeFIf0zDaK/Z3XNGYdz/CF7s3ZLI6cN3Zc+O+fvxb96jf05w/szcX4d/3HiYP79hIwW9uevHsXh/XajX5+M8Jv0gxM5/pcTOfeowgZr997dOX3ovrmRKD847VCuPHEwR1w/gU31zXzv1EM4+fB9OPMPr/DXS0bSZ+du7L97phH+3BH9+OWXhnH4T57h4/134/MjBjBwz9789Mm5vP3Bhlzj1at7V75zyiE0tbQyb9Wm3DI0e+/ag19+6Qj26N2ds299lQP37s2pQz7GnS8v5ktHDuBHZx7O1Q+/w/3T8orxd089hG5dduJ/np7HnS8v5uvHDuK/HpqJKlxxwkEM2qs3t05cyNEDd+f1a06ipVX51bPv8vritRy4V2+EjGZ5+E+e4YA9evH5EQMA+MjrmX7jhIO49+KRtLQq9U0t9O7RlXVbG/n+AzP4/Ij92Ll7F045fB+en1fH1GtPYcmacFPRHvWbWdczo3FeNKQ7P/ryydw9eTG/nLCA/z7jMH4xfj577dKdm754BF/+1L9w9q2v5tJe/JmBXHvG4azcWM/+e/TKOXb37N2dLx45IBdvzg2nsa2xhaN//jw9uu7EbV/+JIf3342rHprFWx+s5+vHHcjXjzuQ3z//HtOWrmPeyk0MG9CXF+bXcXi/3Th64B5M/K9RXHLvVK4943BOPnwfRCQ3ymnUoftwwiF7c+Yn+nHy4fvw19eX0KNrFw7bdze+fuwg/vTK+9xw9se5ftycUD/Hzz8/lGsfzTRRh35sVxZ4Q3AvPXYQlxw7iM/c9GJJmv/9UqbDcdi+uzJ/1Wa+f+ohHH/I3gzbvy8zlm3glhcXcuQBu3PmJ/rRZSdh8S/OQCS/DM7uvUq1k4F79mKf3XoiIlxx/IHs2bs7Zw3rT5edhCVrt3LwPruweM1W3lm+kTVbGrjxqXmc8Yl9+c15wznsx89wxIA+fO6I/vTduTtf+XPGAPSfJw3mYM+q0K8PfDgv9HWojOxWrpX+AXsDfb3fOwOTgc8VxRkBzAMOIqMFjQVuDMlvIDC7KKwrsBgYBHQHZgIfd63jkUceqZUwceLEitL7aWxu0dkrNuj2xmZtbG5JLd+0SfOao1i9uV5bWloDz/31tfd18rurc8drtzToawvX5I6bfelaW1v1qVkf5u7p9sZmbWgqvL8frN0aWo8Jz79YkF82/+aWVm1padW3P1ifC69vatbHZ6zQuyct0kV1m3PhDU0tev+UD7SlpVWXrNmiT7/zYcSVl7Klvkm3NjTFShPG84NH6pZuPfU7Z35P//ipL+pHvftqK6iKqGrmXmSfcWNziz4+Y0Xgc/ho03a9e9IifXH+RyX3R1X15QV1+uGGbYF12N7YXJIm7Flv2NaoD05bpq2twedVVV+Yt0pn+J5Dli31TbqlPn/f1njv1IcbtumNT87R6UvX6eb6Jl2wapNOnDgxV6dpS9bq5vom3d7YrC8tqNNJ79YV5FG3qV4Xr96i81du0mlL1ubOrd3SoPdP/aCkHmHX5uflBXU6bclanbVsg06c/5G2trY6pfOXXd/UrKqq81Zu1NWb63PntjY0aUNTS8k9rORbBqZpWNsfdiLuH3AE8DYwi4z28ZOAOJ8BPuE77gZcFhBvLLASaCLj97jUd+4M4F1gEXBtnDq2JyHSUdjRrrnTXe8BB6jmJ1fn/w44IBel012zA3bN8YgSIqmZs1R1lqdpRMV5tei4Cbg7IN6YiDzGA+MTVtMwdix+/nO4/HLY5jN39uqVCTeMFGhfs+MMw0iXCy+Eu+6CAw7IbHpywAGZ4wsvrHXNjE5Cp1j2xDCMCC680ISGUTVMEzEMwzASY0LEMAzDSIwJEcMwDCMxJkQMwzCMxJgQMQzDMBIjWq1F5tshIrIaKF3r2p29gDVlY3UudrRr3tGuF+yadxQqueYDVDVwefAdSohUiohMU9Wjal2PtmRHu+Yd7XrBrnlHoVrXbOYswzAMIzEmRAzDMIzEmBCJx121rkAN2NGueUe7XrBr3lGoyjWbT8QwDMNIjGkihmEYRmJMiDggIqNFZIGILBSRq2tdn7QQkf1FZKKIzBWROSLybS98DxF5TkTe8/7f3QsXEfmDdx9miUjl+6/WCBHpIiJvi8iT3vEgEXnTu7b7RaS7F97DO17onR9Y04onRET6ishDIjJfROaJyKc7+3MWke967/VsERkrIj0723MWkXtEpE5EZvvCYj9XEfmqF/89EflqnDqYECmDiHQBbgNOB4YAY0RkSHSqDkMz8H1VHQIcA1zpXdvVwAuqejDwgncMmXtwsPd3OXB721c5Nb5NZpfNLP8L/FZVBwPrgUu98EuB9V74b714HZHfA8+o6mHAMDLX3mmfs4jsB3wLOEpVhwJdgAvofM/5XjLbkPuJ9VxFZA8yW5V/ChgJXJcVPE6E7VZlf7mdFD8NTPAdXwNcU+t6VelaHwdOBRYA/bywfsAC7/edwBhf/Fy8jvQHDPA+rpOAJwEhMwmra/EzByYAn/Z+d/XiSa2vIeb19gHeL653Z37OwH7AMmAP77k9CZzWGZ8zRVuJx32uwBjgTl94Qbxyf6aJlCf7MmZZ7oV1Kjz1fQTwJvAxVV3pnVoFfMz73Vnuxe+AHwKt3vGewAZVbfaO/deVu2bv/EYvfkdiELAa+ItnwvuTiPSmEz9nVV0B/Ar4gMxW2xuB6XTu55wl7nOt6HmbEDEQkV2Ah4HvqOom/znNdE06zRA+EfkcUKeq02tdlzakK/BJ4HZVHQFsJW/iADrlc94dOIeMAO0P9KbU7NPpaYvnakKkPCuA/X3HA7ywToGIdCMjQP6pqo94wR+JSD/vfD+gzgvvDPfiM8DZIrIEuI+MSev3QF8Rye706b+u3DV75/sAa9uywimwHFiuqm96xw+RESqd+TmfAryvqqtVtQl4hMyz78zPOUvc51rR8zYhUp6pwMHeqI7uZJxzT9S4TqkgIgL8GZinqr/xnXoCyI7Q+CoZX0k2/CJvlMcxwEaf2twhUNVrVHWAqg4k8yxfVNULgYnAl7xoxdecvRdf8uJ3qB67qq4ClonIoV7QycBcOvFzJmPGOkZEennvefaaO+1z9hH3uU4APisiu3sa3Ge9MDdq7RTqCH/AGcC7wCLg2lrXJ8XrOpaMqjsLmOH9nUHGFvwC8B7wPLCHF1/IjFRbBLxDZuRLza+jgusfBTzp/T4QmAIsBB4EenjhPb3jhd75A2td74TXOhyY5j3rx4DdO/tzBm4A5gOzgb8DPTrbcwbGkvH5NJHROC9N8lyBS7xrXwhcHKcONmPdMAzDSIyZswzDMIzEmBAxDMMwEmNCxDAMw0iMCRHDMAwjMSZEDMMwjMSYEDEMwzASY0LEMAzDSIwJEcMwDCMx/x9RW8G2/ioK7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.loss.plot()\n",
    "plt.scatter([df.loss.argmin()], [df.loss.min()], c='r')\n",
    "# df.tr_loss.plot()\n",
    "plt.yscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8896249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df.loss.rolling(20).mean().plot()\n",
    "plt.scatter([df.loss.argmin()], [df.loss.min()], c='r')\n",
    "df.tr_loss.rolling(20).mean().plot()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b2a32093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df.tr_loss, df.loss, c=(df.loss-df.tr_loss)/df.loss*100)\n",
    "plt.title('Comparison of training and dev losses.\\n Color corresponds to overfitting percentage')\n",
    "plt.colorbar()\n",
    "m = min(df.tr_loss.min(), df.loss.min())\n",
    "M = max(df.tr_loss.max(), df.loss.max())\n",
    "plt.plot([m, M], [m, M], 'k--')\n",
    "plt.xlabel('tr loss')\n",
    "plt.ylabel('dev loss')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5d908644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>tr_loss</th>\n",
       "      <th>params.learning_rate</th>\n",
       "      <th>params.n_estimators</th>\n",
       "      <th>params.objective</th>\n",
       "      <th>params.reg_alpha</th>\n",
       "      <th>params.subsample</th>\n",
       "      <th>train_time</th>\n",
       "      <th>status</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>299221.864446</td>\n",
       "      <td>235478.103361</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1460</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.260258</td>\n",
       "      <td>ok</td>\n",
       "      <td>-235478.103361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>299083.208546</td>\n",
       "      <td>233732.075608</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1610</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.511403</td>\n",
       "      <td>ok</td>\n",
       "      <td>-233732.075608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>299151.785484</td>\n",
       "      <td>233138.195638</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1670</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.728529</td>\n",
       "      <td>ok</td>\n",
       "      <td>-233138.195638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>299161.054625</td>\n",
       "      <td>232546.925179</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1730</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.837287</td>\n",
       "      <td>ok</td>\n",
       "      <td>-232546.925179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>299161.054625</td>\n",
       "      <td>232546.925179</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1730</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.621785</td>\n",
       "      <td>ok</td>\n",
       "      <td>-232546.925179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>299183.581488</td>\n",
       "      <td>232309.084073</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1920</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.071576</td>\n",
       "      <td>ok</td>\n",
       "      <td>-232309.084073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>299102.303181</td>\n",
       "      <td>232090.719539</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1780</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.647600</td>\n",
       "      <td>ok</td>\n",
       "      <td>-232090.719539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>299168.587297</td>\n",
       "      <td>231548.172805</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2000</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.122056</td>\n",
       "      <td>ok</td>\n",
       "      <td>-231548.172805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>299183.105988</td>\n",
       "      <td>231363.701008</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2020</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.9</td>\n",
       "      <td>7.445719</td>\n",
       "      <td>ok</td>\n",
       "      <td>-231363.701008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>299113.721006</td>\n",
       "      <td>231260.397753</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2030</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.143197</td>\n",
       "      <td>ok</td>\n",
       "      <td>-231260.397753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>299127.468663</td>\n",
       "      <td>231202.561658</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2040</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.204910</td>\n",
       "      <td>ok</td>\n",
       "      <td>-231202.561658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>299103.924489</td>\n",
       "      <td>231130.514778</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2050</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.234586</td>\n",
       "      <td>ok</td>\n",
       "      <td>-231130.514778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>299127.127667</td>\n",
       "      <td>230504.145538</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2130</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.340780</td>\n",
       "      <td>ok</td>\n",
       "      <td>-230504.145538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>299124.674781</td>\n",
       "      <td>230447.609101</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2140</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.181825</td>\n",
       "      <td>ok</td>\n",
       "      <td>-230447.609101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>299217.376215</td>\n",
       "      <td>230318.961245</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1940</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.227202</td>\n",
       "      <td>ok</td>\n",
       "      <td>-230318.961245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>299085.649977</td>\n",
       "      <td>230300.867520</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2160</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.346664</td>\n",
       "      <td>ok</td>\n",
       "      <td>-230300.867520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>299087.553423</td>\n",
       "      <td>230196.654260</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.380321</td>\n",
       "      <td>ok</td>\n",
       "      <td>-230196.654260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>299042.412997</td>\n",
       "      <td>230158.005516</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2020</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.070480</td>\n",
       "      <td>ok</td>\n",
       "      <td>-230158.005516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>298997.261117</td>\n",
       "      <td>229913.494211</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2210</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.441961</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229913.494211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>298999.506587</td>\n",
       "      <td>229885.888483</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2060</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.940384</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229885.888483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>299003.323929</td>\n",
       "      <td>229826.983574</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2070</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.039256</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229826.983574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>299075.225318</td>\n",
       "      <td>229752.642758</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2230</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.456400</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229752.642758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>299056.030698</td>\n",
       "      <td>229465.406707</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2260</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.463450</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229465.406707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>298962.620426</td>\n",
       "      <td>229039.443816</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.884760</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229039.443816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>298962.620426</td>\n",
       "      <td>229039.443816</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.703802</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229039.443816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>298962.620421</td>\n",
       "      <td>229039.443760</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.433737</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229039.443760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>298962.620429</td>\n",
       "      <td>229039.443709</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2310</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.487250</td>\n",
       "      <td>ok</td>\n",
       "      <td>-229039.443709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>298969.271321</td>\n",
       "      <td>228985.748815</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2320</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.579816</td>\n",
       "      <td>ok</td>\n",
       "      <td>-228985.748815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>298969.271321</td>\n",
       "      <td>228985.748815</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2320</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.083797</td>\n",
       "      <td>ok</td>\n",
       "      <td>-228985.748815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>299041.961269</td>\n",
       "      <td>228942.422155</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.399201</td>\n",
       "      <td>ok</td>\n",
       "      <td>-228942.422155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss        tr_loss  params.learning_rate  params.n_estimators  \\\n",
       "77   299221.864446  235478.103361                  0.14                 1460   \n",
       "289  299083.208546  233732.075608                  0.14                 1610   \n",
       "117  299151.785484  233138.195638                  0.14                 1670   \n",
       "147  299161.054625  232546.925179                  0.14                 1730   \n",
       "257  299161.054625  232546.925179                  0.14                 1730   \n",
       "500  299183.581488  232309.084073                  0.13                 1920   \n",
       "73   299102.303181  232090.719539                  0.14                 1780   \n",
       "712  299168.587297  231548.172805                  0.13                 2000   \n",
       "559  299183.105988  231363.701008                  0.13                 2020   \n",
       "985  299113.721006  231260.397753                  0.13                 2030   \n",
       "920  299127.468663  231202.561658                  0.13                 2040   \n",
       "680  299103.924489  231130.514778                  0.13                 2050   \n",
       "804  299127.127667  230504.145538                  0.13                 2130   \n",
       "694  299124.674781  230447.609101                  0.13                 2140   \n",
       "548  299217.376215  230318.961245                  0.14                 1940   \n",
       "778  299085.649977  230300.867520                  0.13                 2160   \n",
       "510  299087.553423  230196.654260                  0.13                 2170   \n",
       "68   299042.412997  230158.005516                  0.14                 2020   \n",
       "610  298997.261117  229913.494211                  0.13                 2210   \n",
       "351  298999.506587  229885.888483                  0.14                 2060   \n",
       "327  299003.323929  229826.983574                  0.14                 2070   \n",
       "597  299075.225318  229752.642758                  0.13                 2230   \n",
       "797  299056.030698  229465.406707                  0.13                 2260   \n",
       "746  298962.620426  229039.443816                  0.13                 2310   \n",
       "907  298962.620426  229039.443816                  0.13                 2310   \n",
       "914  298962.620421  229039.443760                  0.13                 2310   \n",
       "822  298962.620429  229039.443709                  0.13                 2310   \n",
       "468  298969.271321  228985.748815                  0.13                 2320   \n",
       "859  298969.271321  228985.748815                  0.13                 2320   \n",
       "314  299041.961269  228942.422155                  0.14                 2170   \n",
       "\n",
       "    params.objective  params.reg_alpha  params.subsample  train_time status  \\\n",
       "77        regression              0.00               1.0    3.260258     ok   \n",
       "289       regression              0.00               1.0    3.511403     ok   \n",
       "117       regression              0.00               1.0    3.728529     ok   \n",
       "147       regression              0.00               1.0    3.837287     ok   \n",
       "257       regression              0.00               1.0    3.621785     ok   \n",
       "500       regression              0.05               0.8    4.071576     ok   \n",
       "73        regression              0.00               1.0    3.647600     ok   \n",
       "712       regression              0.05               0.8    4.122056     ok   \n",
       "559       regression              0.06               0.9    7.445719     ok   \n",
       "985       regression              0.03               0.7    4.143197     ok   \n",
       "920       regression              0.05               0.7    4.204910     ok   \n",
       "680       regression              0.05               0.7    4.234586     ok   \n",
       "804       regression              0.04               0.8    4.340780     ok   \n",
       "694       regression              0.05               0.7    4.181825     ok   \n",
       "548       regression              0.02               0.8    4.227202     ok   \n",
       "778       regression              0.05               0.9    4.346664     ok   \n",
       "510       regression              0.03               0.8    5.380321     ok   \n",
       "68        regression              0.00               0.9    5.070480     ok   \n",
       "610       regression              0.03               0.7    4.441961     ok   \n",
       "351       regression              0.00               0.8    4.940384     ok   \n",
       "327       regression              0.00               0.8    4.039256     ok   \n",
       "597       regression              0.05               0.8    4.456400     ok   \n",
       "797       regression              0.06               0.8    4.463450     ok   \n",
       "746       regression              0.05               0.9    4.884760     ok   \n",
       "907       regression              0.05               0.7    4.703802     ok   \n",
       "914       regression              0.04               0.7    4.433737     ok   \n",
       "822       regression              0.03               0.8    4.487250     ok   \n",
       "468       regression              0.04               0.8    4.579816     ok   \n",
       "859       regression              0.04               0.7    5.083797     ok   \n",
       "314       regression              0.00               0.8    5.399201     ok   \n",
       "\n",
       "             score  \n",
       "77  -235478.103361  \n",
       "289 -233732.075608  \n",
       "117 -233138.195638  \n",
       "147 -232546.925179  \n",
       "257 -232546.925179  \n",
       "500 -232309.084073  \n",
       "73  -232090.719539  \n",
       "712 -231548.172805  \n",
       "559 -231363.701008  \n",
       "985 -231260.397753  \n",
       "920 -231202.561658  \n",
       "680 -231130.514778  \n",
       "804 -230504.145538  \n",
       "694 -230447.609101  \n",
       "548 -230318.961245  \n",
       "778 -230300.867520  \n",
       "510 -230196.654260  \n",
       "68  -230158.005516  \n",
       "610 -229913.494211  \n",
       "351 -229885.888483  \n",
       "327 -229826.983574  \n",
       "597 -229752.642758  \n",
       "797 -229465.406707  \n",
       "746 -229039.443816  \n",
       "907 -229039.443816  \n",
       "914 -229039.443760  \n",
       "822 -229039.443709  \n",
       "468 -228985.748815  \n",
       "859 -228985.748815  \n",
       "314 -228942.422155  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = df[df.loss < df.loss.min() * 1.001].sort_values('tr_loss', ascending=False).head(30)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2438801e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tr_loss': 235478.10336061733,\n",
       " 'dev_loss': 299221.8644455432,\n",
       " 'test_loss': 265307.85308950214}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {k.replace('params.', ''):v for k, v in best.iloc[0].to_dict().items() if 'params.' in k}\n",
    "rf_pipe = make_pipeline(\n",
    "    features_pipe,\n",
    "    lgb.LGBMRegressor(random_state=42, **params)\n",
    ")\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "best_model_performance = dict(\n",
    "    tr_loss=rmse(y_train, rf_pipe.predict(X_train)), \n",
    "    dev_loss=rmse(y_dev, rf_pipe.predict(X_dev)),\n",
    "    test_loss=rmse(y_test, rf_pipe.predict(X_test)), \n",
    ")\n",
    "best_model_performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
